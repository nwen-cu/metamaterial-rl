{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning- and FEM-based Inverse Design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable, Any\n",
    "from typing import Tuple, List, Set, Dict\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `multiprocess` is used instead of built-in `multiprocessing`, to support multiprocessing in Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "from multiprocess import Process, Pool, Queue, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing ...\n",
      "numthread = 1\n"
     ]
    }
   ],
   "source": [
    "import getfem as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from pyvirtualdisplay.display import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocess.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.29273986816406"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all memory using os.popen()\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "mem_gib = mem_bytes/(1024.**3)\n",
    "mem_gib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesla V100S-PCIE-32GB', 'Tesla V100S-PCIE-32GB']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current computing device: cuda\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Current computing device:', cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocess.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(*args):\n",
    "    with open('debug.log', 'a') as fp:\n",
    "        print(*args, file=fp)\n",
    "debug('New Session Started Here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'fem_err.log': No such file or directory\n",
      "rm: cannot remove 'render_err.log': No such file or directory\n",
      "rm: cannot remove 'reward.log': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash clean.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Classes\n",
    "State = Tensor\n",
    "    \n",
    "Action = Tuple[Tuple[int, int], float] \n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    state: State\n",
    "    action_index: int\n",
    "    reward: float\n",
    "    next_state: State\n",
    "        \n",
    "class FEMTask(NamedTuple):\n",
    "    state: State\n",
    "    state_key: str\n",
    "    action_index: int\n",
    "    next_state: State\n",
    "    next_state_key: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardEnv Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridHoleBoardEnv():\n",
    "    def __init__(self, \n",
    "                 size: Tuple[float, float], \n",
    "                 grid_size: Tuple[int, int],\n",
    "                 holes_disabled: Optional[Set[Tuple[int, int]]] = None) -> None:\n",
    "        self.size: Tuple[float, float] = size\n",
    "        self.grid_size: Tuple[int, int] = grid_size\n",
    "        self.cell_size: Tuple[float, float] = (size[0] / grid_size[0], size[1] / grid_size[1])\n",
    "        # (x, y) -> size\n",
    "        self.holes: Tensor = torch.zeros(self.grid_size, device=cuda)\n",
    "            \n",
    "        # (x, y) -> (x_coord, y_coord)\n",
    "        self.holes_center: Tensor = torch.zeros((*self.grid_size, 2))\n",
    "            \n",
    "        self.holes_disabled: Set[Tuple(int, int)] = holes_disabled if holes_disabled else {}\n",
    "            \n",
    "        self.action_space: List[Tuple[Tuple[int, int], float]] = list()\n",
    "            \n",
    "        for x in range(self.grid_size[0]):\n",
    "            for y in range(self.grid_size[1]):\n",
    "                self.holes_center[x, y, 0] = (x + 0.5) * self.cell_size[0]\n",
    "                self.holes_center[x, y, 1] = (y + 0.5) * self.cell_size[1]\n",
    "                if (x, y) not in self.holes_disabled:\n",
    "                    self.action_space.append(((x, y), 0.5))\n",
    "                    self.action_space.append(((x, y), -0.5))\n",
    "                    \n",
    "    def reset(self) -> None:\n",
    "        self.holes: Tensor = torch.ones(self.grid_size, device=cuda)\n",
    "            \n",
    "    def random(self) -> None:\n",
    "        self.holes: Tensor = torch.rand(self.grid_size, device=cuda) * 3\n",
    "    \n",
    "    def step(self, action: Action) -> None:\n",
    "        (x, y), size_change = action\n",
    "        \n",
    "        self.holes[x, y] += size_change\n",
    "        \n",
    "        self.holes[x, y] = torch.clamp(self.holes[x, y], 0., min(self.cell_size) / 2 - 1)\n",
    "        \n",
    "    def get_state(self) -> State:\n",
    "        return self.holes.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMConfig Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEMPhysic():\n",
    "    #\n",
    "    # Physical parameters\n",
    "    #\n",
    "    epsilon = .02       # Thickness of the plate (cm)\n",
    "    E = 21E6           # Young Modulus (N/cm^2)\n",
    "    nu = 0.3           # Poisson ratio\n",
    "    clambda = E*nu/((1+nu)*(1-2*nu)) # First Lame coefficient (N/cm^2)\n",
    "    cmu = E/(2*(1+nu))               # Second Lame coefficient (N/cm^2)\n",
    "    clambdastar = 2*clambda*cmu/(clambda+2*cmu) # Lame coefficient for Plane stress (N/cm^2)\n",
    "    F = 100E2          # Force density at the right boundary (N/cm^2)\n",
    "    kappa = 4.         # Thermal conductivity (W/(cm K))\n",
    "    D = 10.            # Heat transfer coefficient (W/(K cm^2))\n",
    "    air_temp = 20.     # Temperature of the air in oC.\n",
    "    alpha_th = 16.6E-6 # Thermal expansion coefficient (/K).\n",
    "    T0 = 20.           # Reference temperature in oC.\n",
    "    rho_0 = 1.754E-8   # Resistance temperature coefficient at T0 = 20oC\n",
    "    alpha = 0.0039     # Second resistance temperature coefficient.\n",
    "\n",
    "    #\n",
    "    # Numerical parameters\n",
    "    #\n",
    "    elements_degree = 2       # Degree of the finite element methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardFEMTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridHoleBoardFEMTask(render_queue: Queue,\n",
    "                         result_cache: Dict[str, str],\n",
    "                         task_id: int,\n",
    "                         state_key: str,\n",
    "                         state: State,\n",
    "                         board_size: Tuple[float, float],\n",
    "                         grid_size: Tuple[int, int],\n",
    "                         holes_center: Tensor,\n",
    "                         element_diameter: float,\n",
    "                         fem_physic: FEMPhysic, \n",
    "                         queue_time: datetime) -> None:\n",
    "    import os\n",
    "    import sys\n",
    "    import multiprocess\n",
    "    \n",
    "    devnull = open(os.devnull, 'w')\n",
    "    oldstdout_fno = os.dup(sys.stdout.fileno())\n",
    "    os.dup2(devnull.fileno(), 1)\n",
    "    \n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "    \n",
    "    import getfem as gf\n",
    "    import numpy as np\n",
    "    \n",
    "    from PIL import Image, ImageChops\n",
    "    \n",
    "    import torch\n",
    "    \n",
    "    import pyvista as pv\n",
    "    from pyvirtualdisplay.display import Display\n",
    "      \n",
    "    start_time = datetime.now()\n",
    "    queue_time = datetime.now() - queue_time\n",
    "    \n",
    "    fem_worker = multiprocess.current_process().name\n",
    "    \n",
    "    # Skip if already cached\n",
    "    if state_key in result_cache:\n",
    "        mesh_time = 'Skipped'\n",
    "        solve_time = 'Skipped'\n",
    "        os.dup2(oldstdout_fno, 1)\n",
    "        devnull.close()\n",
    "\n",
    "        render_queue.put((task_id, state_key, \n",
    "                          f'temp/{result_cache[state_key]}.vtk', result_cache[state_key], \n",
    "                          fem_worker, queue_time, mesh_time, solve_time))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Generate Mesh\n",
    "    try:\n",
    "        board = gf.MesherObject('rectangle', [0., 0.], list(board_size))\n",
    "        holes: List[gf.MesherObject] = list()\n",
    "\n",
    "        for x in range(grid_size[0]):\n",
    "            for y in range(grid_size[1]):\n",
    "                center = holes_center[x, y].tolist()\n",
    "                size = state[x, y].item()\n",
    "\n",
    "                if size < 0.01 * element_diameter: continue\n",
    "\n",
    "                holes.append(gf.MesherObject('ball', center, size))\n",
    "\n",
    "        if holes:\n",
    "            holes_union = gf.MesherObject('union', *holes)\n",
    "            mesher = gf.MesherObject('set minus', board, holes_union)\n",
    "        else:\n",
    "            mesher = board\n",
    "\n",
    "        #print('Beginning mesh generation')\n",
    "        gf.util('trace level', 0)   # No trace for mesh generation\n",
    "        mesh = gf.Mesh('generate', mesher, element_diameter, 2)\n",
    "\n",
    "        boundary: Dict[str, int] = dict()\n",
    "\n",
    "        # Boundary of the holes\n",
    "        boundary['HOLE_BOUND'] = 1\n",
    "        mesh.set_region(boundary['HOLE_BOUND'], \n",
    "                        mesh.outer_faces_in_box([1., 1.], \n",
    "                                                [board_size[0] - 1, board_size[1] - 1]))\n",
    "\n",
    "        boundary['LEFT_BOUND'] = 2\n",
    "        mesh.set_region(boundary['LEFT_BOUND'], mesh.outer_faces_with_direction([-1., 0.], 0.01))        \n",
    "\n",
    "        boundary['RIGHT_BOUND'] = 3\n",
    "        mesh.set_region(boundary['RIGHT_BOUND'], mesh.outer_faces_with_direction([ 1., 0.], 0.01)) \n",
    "\n",
    "        boundary['TOP_BOUND'] = 4\n",
    "        mesh.set_region(boundary['TOP_BOUND'], mesh.outer_faces_with_direction([0.,  1.], 0.01)) \n",
    "\n",
    "        boundary['BOTTOM_BOUND'] = 5\n",
    "        mesh.set_region(boundary['BOTTOM_BOUND'], mesh.outer_faces_with_direction([0., -1.], 0.01)) \n",
    "\n",
    "        mesh.region_subtract( boundary['RIGHT_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(  boundary['LEFT_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(   boundary['TOP_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(boundary['BOTTOM_BOUND'], boundary['HOLE_BOUND'])\n",
    "\n",
    "        region_id = 7\n",
    "        for x in range(grid_size[0]):\n",
    "            for y in range(grid_size[1]):\n",
    "                center = holes_center[x, y].tolist()\n",
    "                size = state[x, y].item()\n",
    "                bound_key = f'HOLE{x}_{y}_BOUND'\n",
    "                boundary[bound_key] = region_id\n",
    "                mesh.set_region(boundary[bound_key], \n",
    "                                mesh.outer_faces_in_ball(center, size + 0.01 * element_diameter))\n",
    "                if region_id == 7:\n",
    "                    boundary['HOLE_UNION_BOUND'] = 6\n",
    "                    mesh.set_region(boundary['HOLE_UNION_BOUND'], \n",
    "                                mesh.outer_faces_in_ball(center, size + 0.01 * element_diameter))\n",
    "                else:\n",
    "                    mesh.region_merge(boundary['HOLE_UNION_BOUND'], boundary[bound_key])\n",
    "                region_id += 1\n",
    "\n",
    "        np.testing.assert_array_equal(mesh.region(boundary['HOLE_BOUND']), \n",
    "                                      mesh.region(boundary['HOLE_UNION_BOUND']))\n",
    "        mesh_time = datetime.now() - start_time\n",
    "    except:\n",
    "        mesh_time = 'Failed'\n",
    "        solve_time = 'Skipped'\n",
    "        render_queue.put((task_id, state_key, \n",
    "                          None, None, \n",
    "                          fem_worker, queue_time, mesh_time, solve_time))\n",
    "        os.dup2(oldstdout_fno, 1)\n",
    "        devnull.close()\n",
    "        return\n",
    "    \n",
    "    # Solve\n",
    "    try:\n",
    "        fp = fem_physic\n",
    "\n",
    "        #\n",
    "        # Definition of finite elements methods and integration method\n",
    "        #\n",
    "\n",
    "        mfu = gf.MeshFem(mesh, 2)  # Finite element for the elastic displacement\n",
    "        mfu.set_classical_fem(fp.elements_degree)\n",
    "        mft = gf.MeshFem(mesh, 1)  # Finite element for temperature and electrical field\n",
    "        mft.set_classical_fem(fp.elements_degree)\n",
    "        mfvm = gf.MeshFem(mesh, 1) # Finite element for Von Mises stress interpolation\n",
    "        mfvm.set_classical_discontinuous_fem(fp.elements_degree)\n",
    "        mim = gf.MeshIm(mesh, fp.elements_degree * 2)   # Integration method\n",
    "\n",
    "        md=gf.Model('real');\n",
    "        md.add_fem_variable('u', mfu)       # Displacement of the structure\n",
    "        md.add_fem_variable('theta', mft)   # Temperature\n",
    "        md.add_fem_variable('V', mft)       # Electric potential\n",
    "\n",
    "        # Membrane elastic deformation\n",
    "        md.add_initialized_data('cmu', [fp.cmu])\n",
    "        md.add_initialized_data('clambdastar', [fp.clambdastar])\n",
    "        md.add_isotropic_linearized_elasticity_brick(mim, 'u', 'clambdastar', 'cmu')\n",
    "\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'u', fp.elements_degree - 1, boundary['LEFT_BOUND'])\n",
    "        md.add_initialized_data('Fdata', [fp.F * fp.epsilon, 0])\n",
    "        md.add_source_term_brick(mim, 'u', 'Fdata', boundary['RIGHT_BOUND'])\n",
    "\n",
    "        # Electrical field\n",
    "        sigmaeps = '(eps/(rho_0*(1+alpha*(theta-T0))))'\n",
    "        md.add_initialized_data('eps', [fp.epsilon])\n",
    "        md.add_initialized_data('rho_0', [fp.rho_0])\n",
    "        md.add_initialized_data('alpha', [fp.alpha])\n",
    "        md.add_initialized_data('T0', [fp.T0])\n",
    "        md.add_nonlinear_term(mim, sigmaeps+'*(Grad_V.Grad_Test_V)')\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'V', fp.elements_degree - 1, boundary['RIGHT_BOUND'])\n",
    "        md.add_initialized_data('DdataV', [2.])\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'V', fp.elements_degree - 1, boundary['LEFT_BOUND'], 'DdataV')\n",
    "\n",
    "        # Thermal problem\n",
    "        md.add_initialized_data('kaeps', [fp.kappa * fp.epsilon])\n",
    "        md.add_generic_elliptic_brick(mim, 'theta', 'kaeps')\n",
    "        md.add_initialized_data('D2', [fp.D * 2])\n",
    "        md.add_initialized_data('D2airt', [fp.air_temp * fp.D * 2])\n",
    "        md.add_mass_brick(mim, 'theta', 'D2')\n",
    "        md.add_source_term_brick(mim, 'theta', 'D2airt')\n",
    "        md.add_initialized_data('Deps', [fp.D / fp.epsilon])\n",
    "        md.add_initialized_data('Depsairt', [fp.air_temp * fp.D / fp.epsilon])\n",
    "        md.add_Fourier_Robin_brick(mim, 'theta', 'Deps', boundary['TOP_BOUND'])\n",
    "        md.add_source_term_brick(mim, 'theta', 'Depsairt', boundary['TOP_BOUND'])\n",
    "        md.add_Fourier_Robin_brick(mim, 'theta', 'Deps', boundary['BOTTOM_BOUND'])\n",
    "        md.add_source_term_brick(mim, 'theta', 'Depsairt', boundary['BOTTOM_BOUND'])\n",
    "\n",
    "        # Joule heating term\n",
    "        md.add_nonlinear_term(mim, '-' + sigmaeps + '*Norm_sqr(Grad_V)*Test_theta')\n",
    "\n",
    "        # Thermal expansion term\n",
    "        md.add_initialized_data('beta', [fp.alpha_th * fp.E / (1 - 2 * fp.nu)])\n",
    "        md.add_linear_term(mim, 'beta*(T0-theta)*Trace(Grad_Test_u)')\n",
    "\n",
    "        #\n",
    "        # Model solve\n",
    "        #\n",
    "\n",
    "        md.disable_variable('u')\n",
    "        md.solve('max_res', 1E-9, 'max_iter', 100)\n",
    "\n",
    "        #\n",
    "        # Solution export\n",
    "        #  \n",
    "        THETA = md.variable('theta')\n",
    "\n",
    "        file_id = uuid.uuid4()\n",
    "\n",
    "        vtk_path = f'temp/{file_id}.vtk'\n",
    "        mft.export_to_vtk(vtk_path, mft, THETA, 'Temperature')\n",
    "\n",
    "        solve_time = datetime.now() - start_time - mesh_time\n",
    "    except:\n",
    "        solve_time = 'Failed'\n",
    "        render_queue.put((task_id, state_key,\n",
    "                          None, None,\n",
    "                          fem_worker, queue_time, mesh_time, solve_time))\n",
    "        os.dup2(oldstdout_fno, 1)\n",
    "        devnull.close()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    os.dup2(oldstdout_fno, 1)\n",
    "    devnull.close()\n",
    "    \n",
    "    \n",
    "        \n",
    "    render_queue.put((task_id, state_key, \n",
    "                      vtk_path, file_id, \n",
    "                      fem_worker, queue_time, mesh_time, solve_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RenderTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenderTask(render_queue: Queue, result_queue: Queue, \n",
    "               result_cache: Dict[str, str], result_image_cache: Dict[str, Tensor],\n",
    "               device: torch.device) -> None:\n",
    "    from datetime import datetime\n",
    "    import multiprocess\n",
    "    \n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageChops\n",
    "    \n",
    "    import torch\n",
    "    from torchvision.transforms import PILToTensor\n",
    "    \n",
    "    import pyvista as pv\n",
    "    from pyvirtualdisplay.display import Display\n",
    "    # Render Image    \n",
    "    with Display(visible=0, size=(1280, 1024)) as display:\n",
    "        p = pv.Plotter(off_screen=True, lighting='three lights')\n",
    "        p.enable_3_lights()\n",
    "        if p.scalar_bars:\n",
    "            for sb in list(p.scalar_bars.keys()):\n",
    "                p.remove_scalar_bar(sb)\n",
    "\n",
    "                \n",
    "        to_tensor = PILToTensor()\n",
    "        \n",
    "        render_worker = multiprocess.current_process().name\n",
    "    \n",
    "        # Starting Pipeline render_queue => this => result_queue\n",
    "        while True:\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            task_id, state_key, vtk_path, file_id, fem_worker, queue_time, mesh_time, solve_time = render_queue.get()\n",
    "\n",
    "            if vtk_path is None:\n",
    "                result_cache[state_key] = None\n",
    "                result_image_cache[state_key] = None\n",
    "                result_queue.put((task_id, state_key, None, None, None))\n",
    "                \n",
    "                render_time = 'Skipped'\n",
    "                with open('fem.log', 'a') as log:\n",
    "                    print('========', file=log)\n",
    "                    print(datetime.now(), file=log)\n",
    "                    print('Task', task_id, file=log)\n",
    "                    print('Key', state_key, file=log)\n",
    "                    print('Queue time', queue_time, file=log)\n",
    "                    print('Mesh time', mesh_time, file=log)\n",
    "                    print('Solve time', solve_time, file=log)\n",
    "                    print('Render time', render_time, file=log)\n",
    "                continue\n",
    "                \n",
    "            if state_key in result_image_cache:\n",
    "                result_queue.put((task_id, state_key, vtk_path, f'result/{file_id}.png', result_image_cache[state_key]))\n",
    "                \n",
    "                render_time = 'Skipped'\n",
    "                with open('fem.log', 'a') as log:\n",
    "                    print('========', file=log)\n",
    "                    print(datetime.now(), file=log)\n",
    "                    print('Task', task_id, file=log)\n",
    "                    print('Key', state_key, file=log)\n",
    "                    print('Queue time', queue_time, file=log)\n",
    "                    print('Mesh time', mesh_time, file=log)\n",
    "                    print('Solve time', solve_time, file=log)\n",
    "                    print('Render time', render_time, file=log)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                m = pv.read(vtk_path)\n",
    "\n",
    "                a = p.add_mesh(m, line_width=5, cmap='Greys_r', clim=[20, 60], show_scalar_bar=False)\n",
    "\n",
    "                p.view_xy()\n",
    "\n",
    "                img_path = f'result/{file_id}.png'\n",
    "\n",
    "                img_arr = p.screenshot(filename=img_path, transparent_background=True, window_size=[512, 384])\n",
    "\n",
    "                p.remove_actor(a, render=False)\n",
    "                # Try whether deep clean is nessary for large dataset\n",
    "                #p.deep_clean()\n",
    "\n",
    "                img = Image.fromarray(img_arr)\n",
    "                bg = Image.new(img.mode, img.size, img.getpixel((0,0)))\n",
    "                diff = ImageChops.difference(img, bg)\n",
    "                diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "                bbox = diff.getbbox()\n",
    "                if bbox:\n",
    "                    img = img.crop(bbox)\n",
    "                img_tensor = to_tensor(img).to(device=device, dtype=torch.float)\n",
    "                \n",
    "                result_cache[state_key] = file_id\n",
    "                result_image_cache[state_key] = img_tensor\n",
    "\n",
    "                render_time = datetime.now() - start_time\n",
    "                \n",
    "            except Exception as e:\n",
    "                img_path = None\n",
    "                img_tensor = None\n",
    "                render_time = 'Failed'\n",
    "                result_cache[state_key] = None\n",
    "                result_image_cache[state_key] = None\n",
    "                with open('render_err.log', 'a') as log:\n",
    "                    print('========', file=log)\n",
    "                    print(datetime.now(), file=log)\n",
    "                    print('Task', task_id, file=log)\n",
    "                    print('Key', state_key, file=log)\n",
    "                    print('Temp file', vtk_path, file=log)\n",
    "                    print('Error', e, file=log)\n",
    "\n",
    "            result_queue.put((task_id, state_key, vtk_path, img_path, img_tensor))\n",
    "\n",
    "            with open('fem.log', 'a') as log:\n",
    "                print('========', file=log)\n",
    "                print(datetime.now(), file=log)\n",
    "                print('Task', task_id, file=log)\n",
    "                print('Key', state_key, file=log)\n",
    "                print('Temp file', vtk_path, file=log)\n",
    "                print('Result file', img_path, file=log)\n",
    "                print('FEM worker', fem_worker, file=log)\n",
    "                print('Render worker', render_worker, file=log)\n",
    "                print('Queue time', queue_time, file=log)\n",
    "                print('Mesh time', mesh_time, file=log)\n",
    "                print('Solve time', solve_time, file=log)\n",
    "                print('Render time', render_time, file=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardFEMSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridHoleBoardFEMSimulator():\n",
    "    def __init__(self, \n",
    "                 environment: GridHoleBoardEnv, \n",
    "                 *, \n",
    "                 element_diameter: float = 2, \n",
    "                 fem_task_pool_size: int = 32,\n",
    "                 render_task_pool_size: int = 8) -> None:\n",
    "        self.environment = environment\n",
    "        self.board_size = environment.size\n",
    "        self.grid_size = environment.grid_size\n",
    "        self.holes_center = environment.holes_center.cpu()\n",
    "        self.element_diameter: float = element_diameter\n",
    "        self.fem_physic: FEMPhysic = FEMPhysic()\n",
    "            \n",
    "        \n",
    "        self.mp_manager: Manager = Manager()\n",
    "            \n",
    "        self.mp_fem_task_pool_size: int = fem_task_pool_size\n",
    "        self.mp_fem_task_pool: Pool = Pool(fem_task_pool_size)\n",
    "            \n",
    "        self.mp_render_task_pool_size: int = render_task_pool_size\n",
    "        self.mp_render_task_pool: Pool = Pool(render_task_pool_size)\n",
    "            \n",
    "        self.mp_render_queue: Queue = self.mp_manager.Queue()\n",
    "        self.mp_result_queue: Queue = self.mp_manager.Queue()\n",
    "            \n",
    "        self.mp_result_cache: Dict[str, str] = self.mp_manager.dict()\n",
    "        self.mp_result_image_cache: Dict[str, Tensor] = self.mp_manager.dict()\n",
    "            \n",
    "        def render_err_callback(err):\n",
    "            with open('render_err.log', 'a') as log:\n",
    "                print('========', file=log)\n",
    "                print(datetime.now(), file=log)\n",
    "                print(err, file=log)\n",
    "            # TODO Restarting render process after failed\n",
    "            \n",
    "        self.mp_render_task_pool.starmap_async(RenderTask,\n",
    "                                               [(self.mp_render_queue, self.mp_result_queue, \n",
    "                                                 self.mp_result_cache, self.mp_result_image_cache,\n",
    "                                                 cuda)] \n",
    "                                               * self.mp_render_task_pool_size,\n",
    "                                               error_callback=render_err_callback)\n",
    "            \n",
    "            \n",
    "    def clear_image_cache(self):\n",
    "        self.mp_result_image_cache.clear()\n",
    "        \n",
    "    def terminate_current_queue(self):\n",
    "        self.mp_fem_task_pool.terminate()\n",
    "\n",
    "    def reset_task_pool(self):\n",
    "        self.mp_fem_task_pool = Pool(self.mp_fem_task_pool_size)  \n",
    "   \n",
    "    def queue(self, task_id: int, state_key: str, state: State) -> None:\n",
    "        queue_time = datetime.now()\n",
    "        def err_callback(err):\n",
    "            with open('fem_err.log', 'a') as log:\n",
    "                print('========', file=log)\n",
    "                print(datetime.now(), file=log)\n",
    "                print('Task', task_id, file=log)\n",
    "                print('Key', state_key, file=log)\n",
    "                print('State', state, file=log)\n",
    "                print(err, file=log)\n",
    "                \n",
    "        self.mp_fem_task_pool.apply_async(func=GridHoleBoardFEMTask,\n",
    "                                          args=(self.mp_render_queue, \n",
    "                                                self.mp_result_cache,\n",
    "                                                task_id,\n",
    "                                                state_key, state.cpu(), \n",
    "                                                self.board_size, self.grid_size, self.holes_center, \n",
    "                                                self.element_diameter, self.fem_physic, queue_time), \n",
    "                                          error_callback=err_callback)\n",
    "    \n",
    "    def complete(self):\n",
    "        self.mp_fem_task_pool.close()\n",
    "        self.mp_fem_task_pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.\n",
    "epsilon_decay = .995\n",
    "lr = .001\n",
    "replay_batch_size = 32\n",
    "target_update_interval = 10\n",
    "\n",
    "final_threshold = 0.01\n",
    "final_step_threshold = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state = torch.tensor([0, 3, 0, 3, \n",
    "                             0, 3, 0, 3, \n",
    "                             0, 3, 0, 3], device=cuda)\n",
    "\n",
    "def MockRewardFunc(state: State, action_index: int, next_state: State) -> float:\n",
    "    loss = nn.MSELoss(reduction='sum')\n",
    "    return float(loss(state.flatten(), target_state)) - float(loss(next_state.flatten(), target_state))\n",
    "\n",
    "def MockFinalFunc(episode: int, step: int, state: State, action_index: int, reward: float, next_state: State):\n",
    "    loss = nn.MSELoss(reduction='sum')\n",
    "    if float(loss(state.flatten(), target_state)) < final_threshold or step >= final_step_threshold:\n",
    "        return state, action_index, reward, None\n",
    "    return state, action_index, reward, next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEM-based Reward & Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEMReward():\n",
    "    def __init__(self, \n",
    "                 target: str, \n",
    "                 compare_spot: List[Tuple[int, int]],\n",
    "                 max_step: int = 1000,\n",
    "                 goal_reward: float = 10000.,\n",
    "                 final_state_threshold: float = 0.01,\n",
    "                 invalid_state_penalty: float = -100.,\n",
    "                 revalid_state_reward: float = 50.) -> None:\n",
    "        self.target_image = PILToTensor()(Image.open(target))[0].to(device=cuda, dtype=torch.float)\n",
    "                                       \n",
    "        # Create a bool mask for extracting the comparing points\n",
    "        self.compare_spot = compare_spot\n",
    "        self.compare_mask = torch.BoolTensor(*(self.target_image.size())).fill_(False).to(cuda)\n",
    "        for spot in compare_spot:\n",
    "            self.compare_mask[spot] = True\n",
    "            \n",
    "        self.target_spot = self.target_image[self.compare_mask]\n",
    "            \n",
    "        self.loss = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "        \n",
    "        self.goal_reward = goal_reward\n",
    "        self.final_state_threshold = final_state_threshold\n",
    "        self.invalid_state_penalty = invalid_state_penalty\n",
    "        self.revalid_state_reward = revalid_state_reward\n",
    "        \n",
    "        \n",
    "    def __call__(self, step: int, state_fem_image: Optional[Tensor], \n",
    "                 action_index: int, next_state_fem_image: Optional[Tensor]):\n",
    "        \n",
    "        reward = 0.\n",
    "        final = False\n",
    "        \n",
    "        if state_fem_image is not None:\n",
    "            state_spot = state_fem_image[0][self.compare_mask]\n",
    "            error = float(self.loss(state_spot, self.target_spot))\n",
    "            final = error <= self.final_state_threshold           \n",
    "        else:\n",
    "            final = False\n",
    "            \n",
    "        if next_state_fem_image is not None:\n",
    "            next_state_spot = next_state_fem_image[0][self.compare_mask]\n",
    "            next_error = float(self.loss(next_state_spot, self.target_spot))\n",
    "            if next_error <= self.final_state_threshold:\n",
    "                reward += self.goal_reward\n",
    "            \n",
    "        if state_fem_image is not None:\n",
    "            if next_state_fem_image is not None:\n",
    "                # Regular loss-based reward for valid-to-valid move\n",
    "                reward += error - next_error\n",
    "            else:\n",
    "                # Give penalty if step into invalid state(unsolvable fem)\n",
    "                reward += self.invalid_state_penalty\n",
    "        else:\n",
    "            if next_state_fem_image is not None:\n",
    "                # Give a one-time reward for backing to a valid state, should be small\n",
    "                reward += self.revalid_state_reward\n",
    "            else:\n",
    "                # Give penalty for each step in invalid states\n",
    "                reward += self.invalid_state_penalty\n",
    "                \n",
    "        with open('reward.log', 'a') as log:\n",
    "            print(f'{reward:.3f}', end='\\t', file=log)\n",
    "            if state_fem_image is not None:\n",
    "                print(f'{error:.3f}', end='\\t', file=log)\n",
    "            else:\n",
    "                print(f'      ', end='\\t', file=log)\n",
    "            if next_state_fem_image is not None:\n",
    "                print(f'{next_error:.3f}', end='\\t', file=log)\n",
    "            else:\n",
    "                print(f'      ', end='\\t', file=log)\n",
    "            print(file=log)\n",
    "        return reward, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Network Container    \n",
    "class Model():\n",
    "    def __init__(self, network: nn.Module, loss_func: _Loss, optimizer: Optimizer):\n",
    "        self.network = network\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def __call__(self, network_input: Tensor) -> Tensor:\n",
    "        return self.network(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNet(state_size: int = 12, action_number: int = 24, target_network: bool = False):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(state_size, 100, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 200, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, action_number, device=cuda),\n",
    "    )\n",
    "    if target_network:\n",
    "        return Model(network=net, loss_func=None, optimizer=None)\n",
    "    else:\n",
    "        return Model(network=net, loss_func=nn.SmoothL1Loss(), optimizer=torch.optim.Adam(net.parameters(), 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory: deque = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment: GridHoleBoardEnv, \n",
    "                 fem_simulator: GridHoleBoardFEMSimulator,\n",
    "                 fem_reward_func: Callable[[Tensor, int, Tensor], float],\n",
    "                 q_network: nn.Module, \n",
    "                 target_network: nn.Module, \n",
    "                 target_update_interval: int = 100, \n",
    "                 optimization_epoch: int = 1,\n",
    "                 experience_replay: ReplayMemory = ReplayMemory(10000), \n",
    "                 replay_batch_size: int = 32, \n",
    "                 discount_factor: float = 0.9,\n",
    "                 explore_factor_start: float = 1,\n",
    "                 explore_factor_end: float = 0.05,\n",
    "                 explore_factor_decay: float = 200,\n",
    "                ) -> None:\n",
    "        self.environment: GridHoleBoardEnv = environment\n",
    "        self.fem_simulator: GridHoleBoardThermalSimulator = fem_simulator\n",
    "        self.fem_reward_func: Callable[[Tensor, int, Tensor], float] = fem_reward_func\n",
    "            \n",
    "        self.q_network: Model = q_network\n",
    "        self.target_network: Model = target_network\n",
    "        self.target_update_interval: int = target_update_interval\n",
    "        \n",
    "        self.optimization_epoch: int = optimization_epoch    \n",
    "        self.experience_replay: ReplayMemory = experience_replay\n",
    "        self.replay_batch_size: int = replay_batch_size\n",
    "            \n",
    "        self.discount_factor: float = discount_factor\n",
    "        self.explore_factor_start: float = explore_factor_start\n",
    "        self.explore_factor_end: float = explore_factor_end\n",
    "        self.explore_factor_decay: float = explore_factor_decay\n",
    "        self.explored_step: int = 0\n",
    "            \n",
    "        self.fem_task_buffer: Dict[int, FEMTask] = dict()\n",
    "            \n",
    "        self.running_loss: List[float] = []\n",
    "            \n",
    "            \n",
    "    def select_action(self, disable_explore: bool = False) -> Tuple[State, int]:\n",
    "        state = self.environment.get_state()\n",
    "        explore_factor = self.explore_factor_end \\\n",
    "                            + (self.explore_factor_start - self.explore_factor_end) \\\n",
    "                            * math.exp(-1. * self.explored_step / self.explore_factor_decay)\n",
    "        if random.random() > explore_factor or disable_explore:\n",
    "            prediction = self.q_network(state.flatten())\n",
    "            action_index = prediction.argmax()\n",
    "        else:\n",
    "            action_index = random.randrange(len(self.environment.action_space))\n",
    "        self.explored_step += 1\n",
    "        return state, action_index\n",
    "            \n",
    "    def step(self, episode: int, step: int, disable_explore: bool = False, max_step: int = 1000) -> bool:\n",
    "        state, action_index = self.select_action(disable_explore)\n",
    "\n",
    "        self.environment.step(self.environment.action_space[action_index])\n",
    "        next_state = self.environment.get_state()\n",
    "        \n",
    "        state_key = str(state.flatten().tolist())\n",
    "        next_state_key = str(next_state.flatten().tolist())\n",
    "        \n",
    "        task = FEMTask(state, state_key, action_index, next_state, next_state_key)\n",
    "        self.fem_task_buffer[step] = task\n",
    "        \n",
    "        self.fem_simulator.queue(step, state_key, state)\n",
    "        self.fem_simulator.queue(step, next_state_key, next_state)\n",
    "        \n",
    "        # Return True if the next state is final\n",
    "        return step >= max_step\n",
    "        \n",
    "    def complete_fem_rewarding(self) -> None:\n",
    "        fem = self.fem_simulator    # Create local reference to shorter statements\n",
    "        print('Start computing rewards based on completed FEM simulation')\n",
    "        i = -1\n",
    "        while len(self.fem_task_buffer) > 0:\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print(f'Received result: {i} Remaining transition: {len(self.fem_task_buffer)}')\n",
    "            \n",
    "            task_id, state_key, vtk_path, img_path, img_tensor = fem.mp_result_queue.get()\n",
    "            \n",
    "            if task_id not in self.fem_task_buffer: \n",
    "                continue\n",
    "                \n",
    "            task = self.fem_task_buffer[task_id]\n",
    "            \n",
    "            step = task_id\n",
    "                        \n",
    "            if task.state_key in fem.mp_result_image_cache and task.next_state_key in fem.mp_result_image_cache:\n",
    "                # Remove transition from wait list\n",
    "                self.fem_task_buffer.pop(task_id)\n",
    "                \n",
    "                if fem.mp_result_image_cache[task.state_key] is None or fem.mp_result_image_cache[task.next_state_key] is None:\n",
    "                    continue\n",
    "                    \n",
    "                state_image = fem.mp_result_image_cache.get(task.state_key, None)\n",
    "                next_state_image = fem.mp_result_image_cache.get(task.next_state_key, None)\n",
    "                \n",
    "                reward, final = self.fem_reward_func(step, state_image, task.action_index, next_state_image)\n",
    "                \n",
    "                if final:\n",
    "                    self.experience_replay.push(task.state, task.action_index, reward, None)\n",
    "                    fem.terminate_current_queue()\n",
    "                    print('Early stop triggered, terminating FEM tasks in current episode')\n",
    "                    break\n",
    "                else:\n",
    "                    self.experience_replay.push(task.state, task.action_index, reward, task.next_state)\n",
    "                    \n",
    "        self.fem_task_buffer.clear()\n",
    "        \n",
    "    def optimize(self) -> None:\n",
    "        if len(self.experience_replay) < replay_batch_size: return\n",
    "        \n",
    "        samples = self.experience_replay.sample(self.replay_batch_size)\n",
    "        batch = Transition(*zip(*samples))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                  batch.next_state)), device=cuda, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s.flatten() for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "\n",
    "        state_batch = torch.stack([s.flatten() for s in batch.state])\n",
    "        action_batch = torch.tensor(batch.action_index, device=cuda).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(batch.reward, device=cuda)\n",
    "\n",
    "        state_action_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "        next_state_values = torch.zeros(self.replay_batch_size, device=cuda)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        loss = self.q_network.loss_func(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.q_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(f'Optimization loss {loss}')\n",
    "        self.running_loss.append(float(loss))\n",
    "        for param in self.q_network.network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.q_network.optimizer.step()\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        self.target_network.network.load_state_dict(self.q_network.network.state_dict())\n",
    "\n",
    "    def train(self, episode: int = 1000, step_per_episode: int = 1000) -> None:\n",
    "        shortest_path = 10000\n",
    "        for e in range(episode):\n",
    "            start_time = datetime.now()\n",
    "            self.environment.reset()\n",
    "#             for step_num in itertools.count():\n",
    "#                 if step_num % 100 == 0:\n",
    "#                     print(f'Episode {e} running, Current step {step_num}', end='\\r\\n')\n",
    "#                     pass\n",
    "#                 if self.step(e, step_num):\n",
    "#                     print(f'Episode {e} completed in {step_num} steps                    ', end='\\r\\n')\n",
    "#                     if step_num < shortest_path:\n",
    "#                         shortest_path = step_num\n",
    "#                         print(f'Shorter path found in episode {e} with {shortest_path} steps       ')\n",
    "#                     break\n",
    "            # Use fixed number of steps in each episode\n",
    "            for step_num in range(step_per_episode):\n",
    "                self.step(e, step_num)\n",
    "            print(f'Episode {e} completed')\n",
    "            self.complete_fem_rewarding()\n",
    "            for i in range(self.optimization_epoch):\n",
    "                self.optimize()\n",
    "            self.fem_simulator.clear_image_cache()\n",
    "            self.fem_simulator.reset_task_pool()\n",
    "            if e % self.target_update_interval == 0:\n",
    "                #print('Target network update                 ')\n",
    "                self.update_target_network()\n",
    "                \n",
    "            end_time = datetime.now()\n",
    "            print(f'Episode {e} completed in {end_time - start_time}')\n",
    "                \n",
    "    def generate(self, step: int = 100) -> State:\n",
    "        self.environment.reset()\n",
    "        for i in range(step):\n",
    "            if self.step(100000000, i):\n",
    "                print(f'Completed in {step} steps                    ', end='\\r\\n')\n",
    "                break\n",
    "        return self.environment.get_state(), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridHoleBoardEnv(size=(80, 60), grid_size=(4, 3))\n",
    "\n",
    "fem = GridHoleBoardFEMSimulator(env, fem_task_pool_size=32, render_task_pool_size=8)\n",
    "\n",
    "reward_func = FEMReward('target.png', [(73, 36),\n",
    "                                       (147, 36),\n",
    "                                       (73, 110),\n",
    "                                       (147, 110),\n",
    "                                       (73, 184),\n",
    "                                       (147, 184),\n",
    "                                       (73, 258),\n",
    "                                       (147, 258)])\n",
    "\n",
    "agent = Agent(env, fem, reward_func, QNet(12, 24), QNet(12, 24, target_network=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 749\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 248\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 48\n",
      "Episode 0 completed in 0:05:15.461897\n",
      "Episode 1 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 851\n",
      "Received result: 400 Remaining transition: 798\n",
      "Received result: 500 Remaining transition: 747\n",
      "Received result: 600 Remaining transition: 698\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 551\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 448\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 347\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 256\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 48\n",
      "Episode 1 completed in 0:02:49.703798\n",
      "Episode 2 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 598\n",
      "Received result: 900 Remaining transition: 551\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 147\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 47\n",
      "Episode 2 completed in 0:02:50.071662\n",
      "Episode 3 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 847\n",
      "Received result: 400 Remaining transition: 802\n",
      "Received result: 500 Remaining transition: 751\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 602\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 351\n",
      "Received result: 1400 Remaining transition: 302\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 151\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 3 completed in 0:03:32.616340\n",
      "Episode 4 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 953\n",
      "Received result: 200 Remaining transition: 902\n",
      "Received result: 300 Remaining transition: 852\n",
      "Received result: 400 Remaining transition: 802\n",
      "Received result: 500 Remaining transition: 754\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 552\n",
      "Received result: 1000 Remaining transition: 502\n",
      "Received result: 1100 Remaining transition: 452\n",
      "Received result: 1200 Remaining transition: 403\n",
      "Received result: 1300 Remaining transition: 353\n",
      "Received result: 1400 Remaining transition: 302\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 204\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 102\n",
      "Received result: 1900 Remaining transition: 52\n",
      "Received result: 2000 Remaining transition: 4\n",
      "Episode 4 completed in 0:05:53.856036\n",
      "Episode 5 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 947\n",
      "Received result: 200 Remaining transition: 899\n",
      "Received result: 300 Remaining transition: 851\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 747\n",
      "Received result: 600 Remaining transition: 698\n",
      "Received result: 700 Remaining transition: 647\n",
      "Received result: 800 Remaining transition: 598\n",
      "Received result: 900 Remaining transition: 548\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 448\n",
      "Received result: 1200 Remaining transition: 400\n",
      "Received result: 1300 Remaining transition: 348\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 49\n",
      "Episode 5 completed in 0:09:08.236063\n",
      "Episode 6 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 849\n",
      "Received result: 400 Remaining transition: 801\n",
      "Received result: 500 Remaining transition: 748\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 402\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 248\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Episode 6 completed in 0:09:47.122508\n",
      "Episode 7 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 804\n",
      "Received result: 500 Remaining transition: 751\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 452\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 351\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 198\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 52\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 7 completed in 0:08:46.001150\n",
      "Episode 8 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 899\n",
      "Received result: 300 Remaining transition: 848\n",
      "Received result: 400 Remaining transition: 800\n",
      "Received result: 500 Remaining transition: 749\n",
      "Received result: 600 Remaining transition: 698\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 548\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 397\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 248\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 49\n",
      "Episode 8 completed in 0:06:03.210970\n",
      "Episode 9 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 946\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 848\n",
      "Received result: 400 Remaining transition: 800\n",
      "Received result: 500 Remaining transition: 748\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 498\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 49\n",
      "Episode 9 completed in 0:12:01.796035\n",
      "Episode 10 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 899\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 801\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 698\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 548\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 98\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 10 completed in 0:02:34.557664\n",
      "Episode 11 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 949\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 848\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 698\n",
      "Received result: 700 Remaining transition: 652\n",
      "Received result: 800 Remaining transition: 601\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 351\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 151\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 52\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 11 completed in 0:17:21.772725\n",
      "Episode 12 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 949\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 848\n",
      "Received result: 400 Remaining transition: 798\n",
      "Received result: 500 Remaining transition: 747\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 547\n",
      "Received result: 1000 Remaining transition: 498\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Episode 12 completed in 0:02:38.242260\n",
      "Episode 13 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 849\n",
      "Received result: 400 Remaining transition: 802\n",
      "Received result: 500 Remaining transition: 751\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 599\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 498\n",
      "Received result: 1100 Remaining transition: 447\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 298\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 98\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Received result: 2000 Remaining transition: 2\n",
      "Episode 13 completed in 0:04:52.302453\n",
      "Episode 14 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 896\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 798\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 646\n",
      "Received result: 800 Remaining transition: 598\n",
      "Received result: 900 Remaining transition: 548\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 397\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 247\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 14 completed in 0:16:18.002780\n",
      "Episode 15 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 899\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 599\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 398\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 48\n",
      "Episode 15 completed in 0:06:06.064295\n",
      "Episode 16 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 802\n",
      "Received result: 500 Remaining transition: 752\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 653\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 502\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 402\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 151\n",
      "Received result: 1800 Remaining transition: 98\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Received result: 2000 Remaining transition: 3\n",
      "Episode 16 completed in 0:17:53.752197\n",
      "Episode 17 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 848\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 749\n",
      "Received result: 600 Remaining transition: 697\n",
      "Received result: 700 Remaining transition: 646\n",
      "Received result: 800 Remaining transition: 599\n",
      "Received result: 900 Remaining transition: 547\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 347\n",
      "Received result: 1400 Remaining transition: 297\n",
      "Received result: 1500 Remaining transition: 247\n",
      "Received result: 1600 Remaining transition: 198\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 100\n",
      "Received result: 1900 Remaining transition: 48\n",
      "Episode 17 completed in 0:10:30.558652\n",
      "Episode 18 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 952\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 852\n",
      "Received result: 400 Remaining transition: 803\n",
      "Received result: 500 Remaining transition: 752\n",
      "Received result: 600 Remaining transition: 702\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 601\n",
      "Received result: 900 Remaining transition: 552\n",
      "Received result: 1000 Remaining transition: 503\n",
      "Received result: 1100 Remaining transition: 453\n",
      "Received result: 1200 Remaining transition: 403\n",
      "Received result: 1300 Remaining transition: 353\n",
      "Received result: 1400 Remaining transition: 302\n",
      "Received result: 1500 Remaining transition: 252\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 102\n",
      "Received result: 1900 Remaining transition: 49\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 18 completed in 0:12:20.804682\n",
      "Episode 19 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 800\n",
      "Received result: 500 Remaining transition: 751\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 547\n",
      "Received result: 1000 Remaining transition: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =10180\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =10180\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =10180\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =10180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Received result: 2000 Remaining transition: 2\n",
      "Episode 19 completed in 0:14:36.414716\n",
      "Episode 20 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 849\n",
      "Received result: 400 Remaining transition: 798\n",
      "Received result: 500 Remaining transition: 749\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 648\n",
      "Received result: 800 Remaining transition: 599\n",
      "Received result: 900 Remaining transition: 548\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 398\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 247\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 48\n",
      "Episode 20 completed in 0:04:23.266630\n",
      "Episode 21 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 855\n",
      "Received result: 400 Remaining transition: 805\n",
      "Received result: 500 Remaining transition: 751\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 651\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 497\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 347\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 202\n",
      "Received result: 1700 Remaining transition: 149\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 21 completed in 0:03:10.967368\n",
      "Episode 22 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 956\n",
      "Received result: 200 Remaining transition: 905\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 803\n",
      "Received result: 500 Remaining transition: 752\n",
      "Received result: 600 Remaining transition: 702\n",
      "Received result: 700 Remaining transition: 653\n",
      "Received result: 800 Remaining transition: 602\n",
      "Received result: 900 Remaining transition: 552\n",
      "Received result: 1000 Remaining transition: 503\n",
      "Received result: 1100 Remaining transition: 452\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 355\n",
      "Received result: 1400 Remaining transition: 302\n",
      "Received result: 1500 Remaining transition: 251\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 153\n",
      "Received result: 1800 Remaining transition: 102\n",
      "Received result: 1900 Remaining transition: 53\n",
      "Received result: 2000 Remaining transition: 2\n",
      "Episode 22 completed in 0:03:00.690183\n",
      "Episode 23 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 897\n",
      "Received result: 300 Remaining transition: 849\n",
      "Received result: 400 Remaining transition: 799\n",
      "Received result: 500 Remaining transition: 748\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 648\n",
      "Received result: 800 Remaining transition: 598\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 400\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Episode 23 completed in 0:05:53.118176\n",
      "Episode 24 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 952\n",
      "Received result: 200 Remaining transition: 901\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 798\n",
      "Received result: 500 Remaining transition: 747\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 601\n",
      "Received result: 900 Remaining transition: 550\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 447\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 250\n",
      "Received result: 1600 Remaining transition: 201\n",
      "Received result: 1700 Remaining transition: 151\n",
      "Received result: 1800 Remaining transition: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received result: 1900 Remaining transition: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n",
      "Level 1 Warning in getfem_superlu.cc, line 217: SuperLU solve failed: info =9724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 24 completed in 0:11:08.417564\n",
      "Episode 25 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 899\n",
      "Received result: 300 Remaining transition: 851\n",
      "Received result: 400 Remaining transition: 800\n",
      "Received result: 500 Remaining transition: 747\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 650\n",
      "Received result: 800 Remaining transition: 598\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 446\n",
      "Received result: 1200 Remaining transition: 400\n",
      "Received result: 1300 Remaining transition: 349\n",
      "Received result: 1400 Remaining transition: 299\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 25 completed in 0:05:50.263261\n",
      "Episode 26 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 854\n",
      "Received result: 400 Remaining transition: 803\n",
      "Received result: 500 Remaining transition: 752\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 652\n",
      "Received result: 800 Remaining transition: 602\n",
      "Received result: 900 Remaining transition: 565\n",
      "Received result: 1000 Remaining transition: 507\n",
      "Received result: 1100 Remaining transition: 455\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 252\n",
      "Received result: 1600 Remaining transition: 198\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 49\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 26 completed in 0:06:31.091312\n",
      "Episode 27 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 954\n",
      "Received result: 200 Remaining transition: 902\n",
      "Received result: 300 Remaining transition: 852\n",
      "Received result: 400 Remaining transition: 802\n",
      "Received result: 500 Remaining transition: 758\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 660\n",
      "Received result: 800 Remaining transition: 605\n",
      "Received result: 900 Remaining transition: 551\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 458\n",
      "Received result: 1200 Remaining transition: 407\n",
      "Received result: 1300 Remaining transition: 356\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 197\n",
      "Received result: 1700 Remaining transition: 151\n",
      "Received result: 1800 Remaining transition: 111\n",
      "Received result: 1900 Remaining transition: 58\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 27 completed in 0:02:50.445269\n",
      "Episode 28 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 951\n",
      "Received result: 200 Remaining transition: 902\n",
      "Received result: 300 Remaining transition: 849\n",
      "Received result: 400 Remaining transition: 801\n",
      "Received result: 500 Remaining transition: 752\n",
      "Received result: 600 Remaining transition: 700\n",
      "Received result: 700 Remaining transition: 652\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 551\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 457\n",
      "Received result: 1200 Remaining transition: 409\n",
      "Received result: 1300 Remaining transition: 352\n",
      "Received result: 1400 Remaining transition: 301\n",
      "Received result: 1500 Remaining transition: 248\n",
      "Received result: 1600 Remaining transition: 199\n",
      "Received result: 1700 Remaining transition: 148\n",
      "Received result: 1800 Remaining transition: 99\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Episode 28 completed in 0:04:08.852663\n",
      "Episode 29 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 851\n",
      "Received result: 400 Remaining transition: 797\n",
      "Received result: 500 Remaining transition: 748\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 499\n",
      "Received result: 1100 Remaining transition: 449\n",
      "Received result: 1200 Remaining transition: 399\n",
      "Received result: 1300 Remaining transition: 350\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 200\n",
      "Received result: 1700 Remaining transition: 153\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 51\n",
      "Episode 29 completed in 0:03:17.930732\n",
      "Episode 30 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 950\n",
      "Received result: 200 Remaining transition: 900\n",
      "Received result: 300 Remaining transition: 850\n",
      "Received result: 400 Remaining transition: 801\n",
      "Received result: 500 Remaining transition: 750\n",
      "Received result: 600 Remaining transition: 701\n",
      "Received result: 700 Remaining transition: 652\n",
      "Received result: 800 Remaining transition: 600\n",
      "Received result: 900 Remaining transition: 549\n",
      "Received result: 1000 Remaining transition: 500\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 400\n",
      "Received result: 1300 Remaining transition: 351\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 197\n",
      "Received result: 1700 Remaining transition: 147\n",
      "Received result: 1800 Remaining transition: 101\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Episode 30 completed in 0:03:30.750088\n",
      "Episode 31 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 898\n",
      "Received result: 300 Remaining transition: 851\n",
      "Received result: 400 Remaining transition: 800\n",
      "Received result: 500 Remaining transition: 753\n",
      "Received result: 600 Remaining transition: 699\n",
      "Received result: 700 Remaining transition: 649\n",
      "Received result: 800 Remaining transition: 599\n",
      "Received result: 900 Remaining transition: 551\n",
      "Received result: 1000 Remaining transition: 501\n",
      "Received result: 1100 Remaining transition: 450\n",
      "Received result: 1200 Remaining transition: 401\n",
      "Received result: 1300 Remaining transition: 351\n",
      "Received result: 1400 Remaining transition: 300\n",
      "Received result: 1500 Remaining transition: 249\n",
      "Received result: 1600 Remaining transition: 202\n",
      "Received result: 1700 Remaining transition: 150\n",
      "Received result: 1800 Remaining transition: 103\n",
      "Received result: 1900 Remaining transition: 50\n",
      "Received result: 2000 Remaining transition: 1\n",
      "Episode 31 completed in 0:03:04.610880\n",
      "Episode 32 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Received result: 0 Remaining transition: 1000\n",
      "Received result: 100 Remaining transition: 948\n",
      "Received result: 200 Remaining transition: 896\n"
     ]
    }
   ],
   "source": [
    "#print(agent.generate())\n",
    "\n",
    "agent.train(1000)\n",
    "\n",
    "#print(agent.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(list(range(len(agent.running_loss))), agent.running_loss, s=1, vmin=0, vmax=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridHoleBoardEnv(size=(80, 60), grid_size=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "env.step(((1, 1), 2.5))\n",
    "\n",
    "state = env.get_state()\n",
    "\n",
    "fem = GridHoleBoardFEMSimulator(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(state.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(str(state.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh, boundary = fem.generate_mesh(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = fem.solve(mesh, boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.render_image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.run(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8 MetaMaterial-RL",
   "language": "python",
   "name": "metamtl-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
