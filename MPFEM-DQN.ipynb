{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning- and FEM-based Inverse Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pil-clemson/metamtl-rl/e/METAMTLRL-81\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "exp = neptune.init(project=\"pil-clemson/metamtl-rl\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable, Any\n",
    "from typing import Tuple, List, Set, Dict\n",
    "from typing import NamedTuple\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from types import SimpleNamespace\n",
    "import queue\n",
    "from queue import PriorityQueue\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `multiprocess` is used instead of built-in `multiprocessing`, to support multiprocessing in Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "from multiprocess import Process, Pool, Queue, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing ...\n",
      "numthread = 1\n"
     ]
    }
   ],
   "source": [
    "import getfem as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from pyvirtualdisplay.display import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('CPU Cores:', multiprocess.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.29273986816406"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all memory using os.popen()\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "mem_gib = mem_bytes/(1024.**3)\n",
    "print('Memory size:', int(mem_gib), 'GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesla V100S-PCIE-32GB', 'Tesla V100S-PCIE-32GB']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print('GPUs:', available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current computing device: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cpu') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Current computing device:', cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocess.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Batch Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Batch Logger logs value of one step without specify the step number every time.\n",
    "Also, steps from all episodes are converted to a log-step, based on the episode and step number. In such case, all steps can be plotted on a single series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogger():\n",
    "    def __init__(self, experiment_logger, episode_multiplier: int = 1000) -> None:\n",
    "        self.exp = experiment_logger\n",
    "        self.episode_multiplier: int = episode_multiplier\n",
    "        self.log_step: int = 0\n",
    "    \n",
    "    def step(self, episode: int, step: int) -> None:\n",
    "        self.episode = episode\n",
    "        self.log_step = self.episode_multiplier * episode + step\n",
    "    \n",
    "    def __call__(self, key: str, value: Union[float, str]):\n",
    "        self.exp[key].log(value, step=self.log_step)\n",
    "        \n",
    "    def __setitem__(self, key: str, value: Union[float, str]):\n",
    "        self.exp[key].log(value, step=self.log_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = BatchLogger(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local Log Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'debug.log': No such file or directory\n",
      "rm: cannot remove 'fem_err.log': No such file or directory\n",
      "rm: cannot remove 'fem.log': No such file or directory\n",
      "rm: cannot remove 'render_err.log': No such file or directory\n",
      "rm: cannot remove 'reward.log': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash clean.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & FEM Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters():\n",
    "    # Environment Parameters\n",
    "    ep = {\n",
    "        'size': (80, 60),\n",
    "        'grid_size': (4, 3)\n",
    "    }\n",
    "\n",
    "    # Hyperparameters\n",
    "    hp = {\n",
    "        'fem_task_pool_size': 32,\n",
    "        'render_task_pool_size': 8,\n",
    "\n",
    "        'target_update_interval': 100,\n",
    "        'optimization_iter': 1,\n",
    "        'experience_replay_size': 10000,\n",
    "        'replay_batch_size': 32,\n",
    "        'lr': .001,\n",
    "        'discount_factor': .9,\n",
    "        'explore_factor_start': 1.,\n",
    "        'explore_factor_end': 0.05,\n",
    "        'explore_factor_decay': 200.,\n",
    "\n",
    "        'final_threshold': 0.01,\n",
    "        'final_step_threshold': 100,\n",
    "\n",
    "        'max_episode': 1000,\n",
    "        'max_step_per_episode': 1000\n",
    "    }\n",
    "\n",
    "    #Network Parameters, current for log only, no programatic function\n",
    "    net_params = {\n",
    "        'state_space_size': 12,\n",
    "        'layers': [100, 200],\n",
    "        'action_space_size': 24,\n",
    "        'activation_func': 'ReLU'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    epsilon = .02       # Thickness of the plate (cm)\n",
    "    E = 21E6           # Young Modulus (N/cm^2)\n",
    "    nu = 0.3            # Poisson ratio\n",
    "    clambda = E * nu / ((1 + nu) * (1 - 2 * nu)) # First Lame coefficient (N/cm^2)\n",
    "    cmu = E / (2 * (1 + nu))               # Second Lame coefficient (N/cm^2)\n",
    "    clambdastar = 2 * clambda * cmu / (clambda + 2 * cmu) # Lame coefficient for Plane stress (N/cm^2)\n",
    "    F = 100E2          # Force density at the right boundary (N/cm^2)\n",
    "    kappa = 4.         # Thermal conductivity (W/(cm K))\n",
    "    D = 10.            # Heat transfer coefficient (W/(K cm^2))\n",
    "    air_temp = 20.     # Temperature of the air in oC.\n",
    "    alpha_th = 16.6E-6 # Thermal expansion coefficient (/K).\n",
    "    T0 = 20.           # Reference temperature in oC.\n",
    "    rho_0 = 1.754E-8   # Resistance temperature coefficient at T0 = 20oC\n",
    "    alpha = 0.0039     # Second resistance temperature coefficient.\n",
    "\n",
    "    # FEM Parameters\n",
    "    fp = {\n",
    "        #\n",
    "        # Physical parameters\n",
    "        #\n",
    "        'epsilon': epsilon,       # Thickness of the plate (cm)\n",
    "        'E': E,           # Young Modulus (N/cm^2)\n",
    "        'nu': nu,           # Poisson ratio\n",
    "        'clambda': clambda, # First Lame coefficient (N/cm^2)\n",
    "        'cmu': cmu,               # Second Lame coefficient (N/cm^2)\n",
    "        'clambdastar': clambdastar, # Lame coefficient for Plane stress (N/cm^2)\n",
    "        'F': F,          # Force density at the right boundary (N/cm^2)\n",
    "        'kappa': kappa,         # Thermal conductivity (W/(cm K))\n",
    "        'D': D,            # Heat transfer coefficient (W/(K cm^2))\n",
    "        'air_temp': air_temp,     # Temperature of the air in oC.\n",
    "        'alpha_th': alpha_th, # Thermal expansion coefficient (/K).\n",
    "        'T0': T0,           # Reference temperature in oC.\n",
    "        'rho_0': rho_0,   # Resistance temperature coefficient at T0 = 20oC\n",
    "        'alpha': alpha,     # Second resistance temperature coefficient.\n",
    "\n",
    "        #\n",
    "        # Numerical parameters\n",
    "        #\n",
    "        'elements_degree': 2,       # Degree of the finite element methods\n",
    "        'element_diameter': 2\n",
    "    }\n",
    "\n",
    "    exp['Environment_Parameters'] = ep\n",
    "    exp['Hyperparameters'] = hp\n",
    "    exp['Network'] = net_params\n",
    "    exp['FEM_Parameters'] = fp\n",
    "\n",
    "    return ep, hp, fp\n",
    "\n",
    "ep, hp, fp = parameters()\n",
    "\n",
    "ep = SimpleNamespace(**ep)\n",
    "hp = SimpleNamespace(**hp)\n",
    "fp = SimpleNamespace(**fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Classes\n",
    "State = Tensor\n",
    "\n",
    "Action = Tuple[Tuple[int, int], float]\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    state: State\n",
    "    action_index: int\n",
    "    reward: float\n",
    "    next_state: State\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    Pending = 0\n",
    "    Running = 1\n",
    "    Successful = 2\n",
    "    Skipped = -1\n",
    "    Failed = -2\n",
    "\n",
    "# A FEMTask instance represent the FEM simulation of a RL transition\n",
    "@dataclass(order=True)\n",
    "class FEMTask():\n",
    "    episode: int = field(compare=False)\n",
    "    step: int\n",
    "    state: State = field(compare=False)\n",
    "    state_key: str = field(compare=False)\n",
    "    action_index: int = field(compare=False)\n",
    "    next_state: State = field(compare=False)\n",
    "    next_state_key: str = field(compare=False)\n",
    "\n",
    "\n",
    "\n",
    "# A FEMSubtask instance represent the FEM simulation of a single state\n",
    "class FEMSubtaskLog():\n",
    "    fem_worker: str = None\n",
    "    render_worker: str = None\n",
    "    queue_time: timedelta = None\n",
    "    mesh_time: Union[str, timedelta] = None\n",
    "    solve_time: Union[str, timedelta] = None\n",
    "    render_time: Union[str, timedelta] = None\n",
    "    error_msg: str = ''\n",
    "\n",
    "@dataclass(order=True)\n",
    "class FEMSubtask():\n",
    "    episode: int = field(compare=False)\n",
    "    step: int\n",
    "    state: State = field(compare=False)\n",
    "    state_key: str = field(compare=False)\n",
    "    successful: bool = field(default=False, compare=False, init=False)\n",
    "    file_id: str = field(default='', compare=False, init=False)\n",
    "    vtk_path: str = field(default='', compare=False, init=False)\n",
    "    img_path: str = field(default='', compare=False, init=False)\n",
    "    img_tensor: Tensor = field(default=None, compare=False, init=False, repr=False)\n",
    "    log: FEMSubtaskLog = field(default=FEMSubtaskLog(), compare=False, init=False)\n",
    "\n",
    "@dataclass(order=True)\n",
    "class FEMTaskResult():\n",
    "    episode: int = field(compare=False)\n",
    "    step: int\n",
    "    state: State = field(compare=False)\n",
    "    state_image: Tensor = field(compare=False, repr=False)\n",
    "    action_index: int = field(compare=False)\n",
    "    next_state: State = field(compare=False)\n",
    "    next_state_image: Tensor = field(compare=False, repr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardEnv Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridHoleBoardEnv():\n",
    "    def __init__(self,\n",
    "                 size: Tuple[float, float],\n",
    "                 grid_size: Tuple[int, int],\n",
    "                 holes_disabled: Optional[Set[Tuple[int, int]]] = None) -> None:\n",
    "        self.size: Tuple[float, float] = size\n",
    "        self.grid_size: Tuple[int, int] = grid_size\n",
    "        self.cell_size: Tuple[float, float] = (size[0] / grid_size[0], size[1] / grid_size[1])\n",
    "        # (x, y) -> size\n",
    "        self.holes: Tensor = torch.zeros(self.grid_size, device=cuda)\n",
    "\n",
    "        # (x, y) -> (x_coord, y_coord)\n",
    "        self.holes_center: Tensor = torch.zeros((*self.grid_size, 2))\n",
    "\n",
    "        self.holes_disabled: Set[Tuple(int, int)] = holes_disabled if holes_disabled else {}\n",
    "\n",
    "        self.action_space: List[Tuple[Tuple[int, int], float]] = list()\n",
    "\n",
    "        for x in range(self.grid_size[0]):\n",
    "            for y in range(self.grid_size[1]):\n",
    "                self.holes_center[x, y, 0] = (x + 0.5) * self.cell_size[0]\n",
    "                self.holes_center[x, y, 1] = (y + 0.5) * self.cell_size[1]\n",
    "                if (x, y) not in self.holes_disabled:\n",
    "                    self.action_space.append(((x, y), 0.5))\n",
    "                    self.action_space.append(((x, y), -0.5))\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.holes: Tensor = torch.ones(self.grid_size, device=cuda)\n",
    "\n",
    "    def random(self) -> None:\n",
    "        self.holes: Tensor = torch.rand(self.grid_size, device=cuda) * 3\n",
    "\n",
    "    def step(self, action: Action) -> None:\n",
    "        (x, y), size_change = action\n",
    "\n",
    "        self.holes[x, y] += size_change\n",
    "\n",
    "        self.holes[x, y] = torch.clamp(self.holes[x, y], 0., min(self.cell_size) / 2 - 1)\n",
    "\n",
    "    def get_state(self) -> State:\n",
    "        return self.holes.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocess FEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMConfig Class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class FEMPhysic():\n",
    "    #\n",
    "    # Physical parameters\n",
    "    #\n",
    "    epsilon = .02       # Thickness of the plate (cm)\n",
    "    E = 21E6           # Young Modulus (N/cm^2)\n",
    "    nu = 0.3           # Poisson ratio\n",
    "    clambda = E*nu/((1+nu)*(1-2*nu)) # First Lame coefficient (N/cm^2)\n",
    "    cmu = E/(2*(1+nu))               # Second Lame coefficient (N/cm^2)\n",
    "    clambdastar = 2*clambda*cmu/(clambda+2*cmu) # Lame coefficient for Plane stress (N/cm^2)\n",
    "    F = 100E2          # Force density at the right boundary (N/cm^2)\n",
    "    kappa = 4.         # Thermal conductivity (W/(cm K))\n",
    "    D = 10.            # Heat transfer coefficient (W/(K cm^2))\n",
    "    air_temp = 20.     # Temperature of the air in oC.\n",
    "    alpha_th = 16.6E-6 # Thermal expansion coefficient (/K).\n",
    "    T0 = 20.           # Reference temperature in oC.\n",
    "    rho_0 = 1.754E-8   # Resistance temperature coefficient at T0 = 20oC\n",
    "    alpha = 0.0039     # Second resistance temperature coefficient.\n",
    "\n",
    "    #\n",
    "    # Numerical parameters\n",
    "    #\n",
    "    elements_degree = 2       # Degree of the finite element methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardFEMTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEMWorkerInitializer(mp_render_queue: Queue,\n",
    "                         mp_result_cache: Dict[str, str],\n",
    "                         env_board_size: Tuple[float, float],\n",
    "                         env_grid_size: Tuple[int, int],\n",
    "                         env_holes_center: Tensor,\n",
    "                         shared_fem_parameters: SimpleNamespace) -> None:\n",
    "    global render_queue, result_cache, board_size, grid_size, holes_center, fem_parameters\n",
    "\n",
    "    render_queue = mp_render_queue\n",
    "    result_cache = mp_result_cache\n",
    "    board_size = env_board_size\n",
    "    grid_size = env_grid_size\n",
    "    holes_center = env_holes_center\n",
    "    fem_parameters = shared_fem_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridHoleBoardFEMTask(subtask: FEMSubtask, queue_time: datetime) -> None:\n",
    "    import os\n",
    "    import sys\n",
    "    import multiprocess\n",
    "\n",
    "    devnull = open(os.devnull, 'w')\n",
    "    oldstdout_fno = os.dup(sys.stdout.fileno())\n",
    "    os.dup2(devnull.fileno(), 1)\n",
    "\n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "\n",
    "    import getfem as gf\n",
    "    import numpy as np\n",
    "\n",
    "    from PIL import Image, ImageChops\n",
    "\n",
    "    import torch\n",
    "\n",
    "    import pyvista as pv\n",
    "    from pyvirtualdisplay.display import Display\n",
    "    \n",
    "    # Creating references for shorter notation\n",
    "    state = subtask.state\n",
    "    state_key = subtask.state_key\n",
    "\n",
    "    fp = fem_parameters\n",
    "\n",
    "    # Timer\n",
    "    start_time = datetime.now()\n",
    "    queue_time = datetime.now() - queue_time\n",
    "\n",
    "    fem_worker = multiprocess.current_process().name\n",
    "    \n",
    "    subtask.log.queue_time = queue_time\n",
    "    subtask.log.fem_worker = fem_worker\n",
    "\n",
    "    # Generate Mesh\n",
    "    try:\n",
    "        board = gf.MesherObject('rectangle', [0., 0.], list(board_size))\n",
    "        holes: List[gf.MesherObject] = list()\n",
    "\n",
    "        for x in range(grid_size[0]):\n",
    "            for y in range(grid_size[1]):\n",
    "                center = holes_center[x, y].tolist()\n",
    "                size = state[x, y].item()\n",
    "\n",
    "                if size < 0.01 * fp.element_diameter: continue\n",
    "\n",
    "                holes.append(gf.MesherObject('ball', center, size))\n",
    "\n",
    "        if holes:\n",
    "            holes_union = gf.MesherObject('union', *holes)\n",
    "            mesher = gf.MesherObject('set minus', board, holes_union)\n",
    "        else:\n",
    "            mesher = board\n",
    "\n",
    "        #print('Beginning mesh generation')\n",
    "        gf.util('trace level', 0)   # No trace for mesh generation\n",
    "        mesh = gf.Mesh('generate', mesher, fp.element_diameter, 2)\n",
    "\n",
    "        boundary: Dict[str, int] = dict()\n",
    "\n",
    "        # Boundary of the holes\n",
    "        boundary['HOLE_BOUND'] = 1\n",
    "        mesh.set_region(boundary['HOLE_BOUND'],\n",
    "                        mesh.outer_faces_in_box([1., 1.],\n",
    "                                                [board_size[0] - 1, board_size[1] - 1]))\n",
    "\n",
    "        boundary['LEFT_BOUND'] = 2\n",
    "        mesh.set_region(boundary['LEFT_BOUND'], mesh.outer_faces_with_direction([-1., 0.], 0.01))\n",
    "\n",
    "        boundary['RIGHT_BOUND'] = 3\n",
    "        mesh.set_region(boundary['RIGHT_BOUND'], mesh.outer_faces_with_direction([ 1., 0.], 0.01))\n",
    "\n",
    "        boundary['TOP_BOUND'] = 4\n",
    "        mesh.set_region(boundary['TOP_BOUND'], mesh.outer_faces_with_direction([0.,  1.], 0.01))\n",
    "\n",
    "        boundary['BOTTOM_BOUND'] = 5\n",
    "        mesh.set_region(boundary['BOTTOM_BOUND'], mesh.outer_faces_with_direction([0., -1.], 0.01))\n",
    "\n",
    "        mesh.region_subtract( boundary['RIGHT_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(  boundary['LEFT_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(   boundary['TOP_BOUND'], boundary['HOLE_BOUND'])\n",
    "        mesh.region_subtract(boundary['BOTTOM_BOUND'], boundary['HOLE_BOUND'])\n",
    "\n",
    "        region_id = 7\n",
    "        for x in range(grid_size[0]):\n",
    "            for y in range(grid_size[1]):\n",
    "                center = holes_center[x, y].tolist()\n",
    "                size = state[x, y].item()\n",
    "                bound_key = f'HOLE{x}_{y}_BOUND'\n",
    "                boundary[bound_key] = region_id\n",
    "                mesh.set_region(boundary[bound_key],\n",
    "                                mesh.outer_faces_in_ball(center, size + 0.01 * fp.element_diameter))\n",
    "                if region_id == 7:\n",
    "                    boundary['HOLE_UNION_BOUND'] = 6\n",
    "                    mesh.set_region(boundary['HOLE_UNION_BOUND'],\n",
    "                                mesh.outer_faces_in_ball(center, size + 0.01 * fp.element_diameter))\n",
    "                else:\n",
    "                    mesh.region_merge(boundary['HOLE_UNION_BOUND'], boundary[bound_key])\n",
    "                region_id += 1\n",
    "\n",
    "        np.testing.assert_array_equal(mesh.region(boundary['HOLE_BOUND']),\n",
    "                                      mesh.region(boundary['HOLE_UNION_BOUND']))\n",
    "        \n",
    "        mesh_time = datetime.now() - start_time\n",
    "        subtask.log.mesh_time = mesh_time\n",
    "    except Exception as err:\n",
    "        subtask.log.mesh_time = 'Failed'\n",
    "        subtask.log.solve_time = 'Skipped'\n",
    "        subtask.log.error_msg += str(err) + '\\n'\n",
    "        \n",
    "        render_queue.put(subtask)\n",
    "        \n",
    "        os.dup2(oldstdout_fno, 1)\n",
    "        devnull.close()\n",
    "        return\n",
    "\n",
    "    # Solve\n",
    "    try:\n",
    "\n",
    "\n",
    "        #\n",
    "        # Definition of finite elements methods and integration method\n",
    "        #\n",
    "\n",
    "        mfu = gf.MeshFem(mesh, 2)  # Finite element for the elastic displacement\n",
    "        mfu.set_classical_fem(fp.elements_degree)\n",
    "        mft = gf.MeshFem(mesh, 1)  # Finite element for temperature and electrical field\n",
    "        mft.set_classical_fem(fp.elements_degree)\n",
    "        mfvm = gf.MeshFem(mesh, 1) # Finite element for Von Mises stress interpolation\n",
    "        mfvm.set_classical_discontinuous_fem(fp.elements_degree)\n",
    "        mim = gf.MeshIm(mesh, fp.elements_degree * 2)   # Integration method\n",
    "\n",
    "        md=gf.Model('real');\n",
    "        md.add_fem_variable('u', mfu)       # Displacement of the structure\n",
    "        md.add_fem_variable('theta', mft)   # Temperature\n",
    "        md.add_fem_variable('V', mft)       # Electric potential\n",
    "\n",
    "        # Membrane elastic deformation\n",
    "        md.add_initialized_data('cmu', [fp.cmu])\n",
    "        md.add_initialized_data('clambdastar', [fp.clambdastar])\n",
    "        md.add_isotropic_linearized_elasticity_brick(mim, 'u', 'clambdastar', 'cmu')\n",
    "\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'u', fp.elements_degree - 1, boundary['LEFT_BOUND'])\n",
    "        md.add_initialized_data('Fdata', [fp.F * fp.epsilon, 0])\n",
    "        md.add_source_term_brick(mim, 'u', 'Fdata', boundary['RIGHT_BOUND'])\n",
    "\n",
    "        # Electrical field\n",
    "        sigmaeps = '(eps/(rho_0*(1+alpha*(theta-T0))))'\n",
    "        md.add_initialized_data('eps', [fp.epsilon])\n",
    "        md.add_initialized_data('rho_0', [fp.rho_0])\n",
    "        md.add_initialized_data('alpha', [fp.alpha])\n",
    "        md.add_initialized_data('T0', [fp.T0])\n",
    "        md.add_nonlinear_term(mim, sigmaeps+'*(Grad_V.Grad_Test_V)')\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'V', fp.elements_degree - 1, boundary['RIGHT_BOUND'])\n",
    "        md.add_initialized_data('DdataV', [2.])\n",
    "        md.add_Dirichlet_condition_with_multipliers(mim, 'V', fp.elements_degree - 1, boundary['LEFT_BOUND'], 'DdataV')\n",
    "\n",
    "        # Thermal problem\n",
    "        md.add_initialized_data('kaeps', [fp.kappa * fp.epsilon])\n",
    "        md.add_generic_elliptic_brick(mim, 'theta', 'kaeps')\n",
    "        md.add_initialized_data('D2', [fp.D * 2])\n",
    "        md.add_initialized_data('D2airt', [fp.air_temp * fp.D * 2])\n",
    "        md.add_mass_brick(mim, 'theta', 'D2')\n",
    "        md.add_source_term_brick(mim, 'theta', 'D2airt')\n",
    "        md.add_initialized_data('Deps', [fp.D / fp.epsilon])\n",
    "        md.add_initialized_data('Depsairt', [fp.air_temp * fp.D / fp.epsilon])\n",
    "        md.add_Fourier_Robin_brick(mim, 'theta', 'Deps', boundary['TOP_BOUND'])\n",
    "        md.add_source_term_brick(mim, 'theta', 'Depsairt', boundary['TOP_BOUND'])\n",
    "        md.add_Fourier_Robin_brick(mim, 'theta', 'Deps', boundary['BOTTOM_BOUND'])\n",
    "        md.add_source_term_brick(mim, 'theta', 'Depsairt', boundary['BOTTOM_BOUND'])\n",
    "\n",
    "        # Joule heating term\n",
    "        md.add_nonlinear_term(mim, '-' + sigmaeps + '*Norm_sqr(Grad_V)*Test_theta')\n",
    "\n",
    "        # Thermal expansion term\n",
    "        md.add_initialized_data('beta', [fp.alpha_th * fp.E / (1 - 2 * fp.nu)])\n",
    "        md.add_linear_term(mim, 'beta*(T0-theta)*Trace(Grad_Test_u)')\n",
    "\n",
    "        #\n",
    "        # Model solve\n",
    "        #\n",
    "\n",
    "        md.disable_variable('u')\n",
    "        md.solve('max_res', 1E-9, 'max_iter', 100)\n",
    "\n",
    "        #\n",
    "        # Solution export\n",
    "        #\n",
    "        THETA = md.variable('theta')\n",
    "\n",
    "        file_id = uuid.uuid4()\n",
    "\n",
    "        vtk_path = f'temp/{file_id}.vtk'\n",
    "        mft.export_to_vtk(vtk_path, mft, THETA, 'Temperature')\n",
    "\n",
    "        solve_time = datetime.now() - start_time - mesh_time\n",
    "        subtask.log.solve_time = solve_time\n",
    "    except Exception as err:\n",
    "        subtask.log.solve_time = 'Failed'\n",
    "        subtask.log.error_msg += str(err) + '\\n'\n",
    "        \n",
    "        render_queue.put(subtask)\n",
    "        \n",
    "        os.dup2(oldstdout_fno, 1)\n",
    "        devnull.close()\n",
    "        return\n",
    "    \n",
    "    subtask.file_id = file_id\n",
    "    subtask.vtk_path = vtk_path\n",
    "\n",
    "    os.dup2(oldstdout_fno, 1)\n",
    "    devnull.close()\n",
    "\n",
    "    render_queue.put(subtask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RenderTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenderTask(render_queue: Queue, result_queue: Queue,\n",
    "               result_cache: Dict[str, str], result_image_cache: Dict[str, Tensor],\n",
    "               device: torch.device) -> None:\n",
    "    from datetime import datetime\n",
    "    import multiprocess\n",
    "\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageChops\n",
    "\n",
    "    import torch\n",
    "    from torchvision.transforms import PILToTensor\n",
    "\n",
    "    import pyvista as pv\n",
    "    from pyvirtualdisplay.display import Display\n",
    "    # Render Image\n",
    "    with Display(visible=0, size=(1280, 1024)) as display:\n",
    "        p = pv.Plotter(off_screen=True, lighting='three lights')\n",
    "        p.enable_3_lights()\n",
    "        if p.scalar_bars:\n",
    "            for sb in list(p.scalar_bars.keys()):\n",
    "                p.remove_scalar_bar(sb)\n",
    "\n",
    "        to_tensor = PILToTensor()\n",
    "\n",
    "        render_worker = multiprocess.current_process().name\n",
    "\n",
    "\n",
    "        # Starting Pipeline render_queue => this => result_queue\n",
    "        while True:\n",
    "            err = ''\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            subtask = render_queue.get()\n",
    "            \n",
    "            vtk_path = subtask.vtk_path\n",
    "            file_id = subtask.file_id\n",
    "            state_key = subtask.state_key\n",
    "\n",
    "            # Skip if FEM task failed in any step\n",
    "            if vtk_path == '' or file_id == '':\n",
    "                subtask.log.render_time = 'Skipped'\n",
    "                result_queue.put(subtask)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                m = pv.read(vtk_path)\n",
    "\n",
    "                a = p.add_mesh(m, line_width=5, cmap='Greys_r', clim=[20, 60], show_scalar_bar=False)\n",
    "\n",
    "                p.view_xy()\n",
    "\n",
    "                img_path = f'result/{file_id}.png'\n",
    "\n",
    "                img_arr = p.screenshot(filename=None, transparent_background=True, window_size=[512, 384])\n",
    "\n",
    "                p.remove_actor(a, render=False)\n",
    "                # Try whether deep clean is nessary for large dataset\n",
    "                #p.deep_clean()\n",
    "\n",
    "                img = Image.fromarray(img_arr)\n",
    "                bg = Image.new(img.mode, img.size, img.getpixel((0,0)))\n",
    "                diff = ImageChops.difference(img, bg)\n",
    "                diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "                bbox = diff.getbbox()\n",
    "                if bbox:\n",
    "                    img = img.crop(bbox)\n",
    "                    \n",
    "                img.save(img_path)\n",
    "                \n",
    "                img_tensor = to_tensor(img).to(device=device)\n",
    "                #img_tensor = to_tensor(img).to(device=device, dtype=torch.float)\n",
    "\n",
    "                result_cache[state_key] = file_id\n",
    "                result_image_cache[state_key] = img_tensor\n",
    "\n",
    "                render_time = datetime.now() - start_time\n",
    "\n",
    "            except Exception as err:\n",
    "                subtask.log.render_time = 'Failed'\n",
    "\n",
    "                result_queue.put(subtask)\n",
    "\n",
    "            subtask.log.render_time = render_time\n",
    "\n",
    "            subtask.img_path = img_path\n",
    "            subtask.img_tensor = img_tensor\n",
    "\n",
    "            subtask.successful = True\n",
    "            result_queue.put(subtask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridHoleBoardFEMSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridHoleBoardFEMSimulator():\n",
    "    def __init__(self,\n",
    "                 environment: GridHoleBoardEnv,\n",
    "                 *,\n",
    "                 fem_task_pool_size: int = hp.fem_task_pool_size,\n",
    "                 render_task_pool_size: int = hp.render_task_pool_size) -> None:\n",
    "        self.environment = environment\n",
    "        self.board_size = environment.size\n",
    "        self.grid_size = environment.grid_size\n",
    "        self.holes_center = environment.holes_center.cpu()\n",
    "\n",
    "        self.mp_manager: Manager = Manager()\n",
    "\n",
    "        self.mp_render_queue: Queue = self.mp_manager.Queue()\n",
    "        self.mp_result_queue: Queue = self.mp_manager.Queue()\n",
    "\n",
    "        self.mp_result_cache: Dict[str, str] = self.mp_manager.dict()\n",
    "        self.mp_result_image_cache: Dict[str, Tensor] = self.mp_manager.dict()\n",
    "\n",
    "        self.mp_fem_task_pool_size: int = fem_task_pool_size\n",
    "        self.mp_fem_task_pool: Pool = Pool(fem_task_pool_size,\n",
    "                                           initializer=FEMWorkerInitializer,\n",
    "                                           initargs=(self.mp_render_queue,\n",
    "                                                     self.mp_result_cache,\n",
    "                                                     self.board_size,\n",
    "                                                     self.grid_size,\n",
    "                                                     self.holes_center,\n",
    "                                                     fp))\n",
    "\n",
    "        self.mp_render_task_pool_size: int = render_task_pool_size\n",
    "        self.mp_render_task_pool: Pool = Pool(render_task_pool_size)\n",
    "\n",
    "        self.task_list: List[FEMTask] = list()\n",
    "        self.subtask_status: Dict[str, TaskStatus] = dict()    # state_key -> task_status\n",
    "        \n",
    "        self.cache_stat: Dict[str, int] = {'newly_solved': 0, 'same_episode': 0, 'image_cache': 0, 'file_cache': 0}\n",
    "\n",
    "        def render_err_callback(err):\n",
    "            with open('render_err.log', 'a') as log:\n",
    "                print('========', file=log)\n",
    "                print(datetime.now(), file=log)\n",
    "                print(err, file=log)\n",
    "            # TODO Restarting render process after failed\n",
    "\n",
    "        self.mp_render_task_pool.starmap_async(RenderTask,\n",
    "                                               [(self.mp_render_queue, self.mp_result_queue,\n",
    "                                                 self.mp_result_cache, self.mp_result_image_cache,\n",
    "                                                 cuda)]\n",
    "                                               * self.mp_render_task_pool_size,\n",
    "                                               error_callback=render_err_callback)\n",
    "\n",
    "    def clear_image_cache(self):\n",
    "        self.mp_result_image_cache.clear()\n",
    "\n",
    "    def terminate_current_queue(self):\n",
    "        self.mp_fem_task_pool.terminate()\n",
    "\n",
    "    def reset_task_pool(self):\n",
    "        exp['cache_stat'].log(str(self.cache_stat))\n",
    "        self.cache_stat = {'newly_solved': 0, 'same_episode': 0, 'image_cache': 0, 'file_cache': 0}\n",
    "        self.task_list.clear()\n",
    "        self.subtask_status.clear()\n",
    "        self.mp_fem_task_pool = Pool(self.mp_fem_task_pool_size,\n",
    "                                     initializer=FEMWorkerInitializer,\n",
    "                                     initargs=(self.mp_render_queue,\n",
    "                                               self.mp_result_cache,\n",
    "                                               self.board_size,\n",
    "                                               self.grid_size,\n",
    "                                               self.holes_center,\n",
    "                                               fp))\n",
    "\n",
    "    def queue_subtask(self, subtask: FEMSubtask) -> None:\n",
    "        state_key = subtask.state_key\n",
    "        \n",
    "        if state_key not in self.subtask_status:\n",
    "            # Fallback to image cache\n",
    "            if state_key in self.mp_result_image_cache:\n",
    "                exp['state_cache'].log(f'From ImgCache: {state_key}')\n",
    "                self.cache_stat['image_cache'] += 1\n",
    "                self.subtask_status[state_key] = TaskStatus.Successful\n",
    "            else:\n",
    "                # Fallback to file cache\n",
    "                if state_key in self.mp_result_cache:\n",
    "                    exp['state_cache'].log(f'From FileCache: {state_key}')\n",
    "                    self.cache_stat['file_cache'] += 1\n",
    "                    self.load_from_cache(state_key)\n",
    "                else:\n",
    "                    # Run FEM for unseen state\n",
    "                    exp['state_cache'].log(f'Solve New: {state_key}')\n",
    "                    self.cache_stat['newly_solved'] += 1\n",
    "                    self.subtask_status[state_key] = TaskStatus.Pending\n",
    "                    queue_time = datetime.now()\n",
    "                    def err_callback(err):\n",
    "                        with open('fem_err.log', 'a') as log:\n",
    "                            print('========', file=log)\n",
    "                            print(datetime.now(), file=log)\n",
    "                            print(subtask, file=log)\n",
    "                            print(err, file=log)\n",
    "\n",
    "                    self.mp_fem_task_pool.apply_async(func=GridHoleBoardFEMTask,\n",
    "                                                      args=(subtask, queue_time),\n",
    "                                                      error_callback=err_callback)\n",
    "        else:\n",
    "            exp['state_cache'].log(f'From Same Episode: {state_key}')\n",
    "            self.cache_stat['same_episode'] += 1\n",
    "                    \n",
    "\n",
    "    def load_from_cache(self, state_key: str):\n",
    "        to_tensor = PILToTensor()\n",
    "        file_id = self.mp_result_cache[state_key]\n",
    "        img_path = f'result/{file_id}.png'\n",
    "        img = Image.open(img_path)\n",
    "        img_tensor = to_tensor(img).to(device=cuda)\n",
    "        self.mp_result_image_cache[state_key] = img_tensor\n",
    "        self.subtask_status[state_key] = TaskStatus.Successful\n",
    "\n",
    "    def queue_transition(self, episode: int, step: int,\n",
    "                         state: State, action_index: int, next_state: State) -> None:\n",
    "\n",
    "        state_key = str(state.flatten().tolist())\n",
    "        next_state_key = str(next_state.flatten().tolist())\n",
    "\n",
    "        self.queue_subtask(FEMSubtask(episode, step, state, state_key))\n",
    "        self.queue_subtask(FEMSubtask(episode, step, next_state, next_state_key))\n",
    "\n",
    "        task = FEMTask(episode, step, state, state_key, action_index, next_state, next_state_key)\n",
    "\n",
    "        self.task_list.append(task)\n",
    "\n",
    "    def get_transition_result(self) -> Generator[FEMTaskResult, None, None]:\n",
    "        for task in self.task_list:\n",
    "            # Retrieve more subtask results if there is pending subtask in current task\n",
    "            while self.subtask_status[task.state_key] is TaskStatus.Pending \\\n",
    "                    or self.subtask_status[task.next_state_key] is TaskStatus.Pending:\n",
    "                self.retrieve_next_task_result()\n",
    "\n",
    "            result = FEMTaskResult(task.episode, task.step,\n",
    "                                   task.state, None,\n",
    "                                   task.action_index,\n",
    "                                   task.next_state, None)\n",
    "\n",
    "            if self.subtask_status[task.state_key] is TaskStatus.Successful:\n",
    "                result.state_image = self.mp_result_image_cache[task.state_key]\n",
    "\n",
    "            if self.subtask_status[task.next_state_key] is TaskStatus.Successful:\n",
    "                result.next_state_image = self.mp_result_image_cache[task.next_state_key]\n",
    "\n",
    "            yield result\n",
    "\n",
    "    def retrieve_next_task_result(self) -> None:\n",
    "        result: FEMSubtask = fem.mp_result_queue.get(timeout=600)\n",
    "        queue_time = 0. if isinstance(result.log.queue_time, str) else result.log.queue_time.total_seconds()\n",
    "        mesh_time = 0. if isinstance(result.log.mesh_time, str) else result.log.mesh_time.total_seconds()\n",
    "        solve_time = 0. if isinstance(result.log.solve_time, str) else result.log.solve_time.total_seconds()\n",
    "        render_time = 0. if isinstance(result.log.render_time, str) else result.log.render_time.total_seconds()\n",
    "\n",
    "        exp['fem_times/queue_time'].log(queue_time)\n",
    "        exp['fem_times/mesh_time'].log(mesh_time)\n",
    "        exp['fem_times/solve_time'].log(solve_time)\n",
    "        exp['fem_times/render_time'].log(render_time)\n",
    "\n",
    "        if result.successful:\n",
    "            self.subtask_status[result.state_key] = TaskStatus.Successful\n",
    "        else:\n",
    "            self.subtask_status[result.state_key] = TaskStatus.Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEM-based Reward & Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEMReward():\n",
    "    def __init__(self,\n",
    "                 environment: GridHoleBoardEnv,\n",
    "                 target: str,\n",
    "                 compare_spot: List[Tuple[int, int]],\n",
    "                 max_step: int = 1000,\n",
    "                 goal_reward: float = 10000.,\n",
    "                 final_state_error_threshold: float = 0.01,\n",
    "                 out_of_bound_penalty: float = -1000) -> None:\n",
    "        self.environment: GridHoleBoardEnv = environment\n",
    "        self.target_image: Tensor = PILToTensor()(Image.open(target))[0].to(device=cuda, dtype=torch.float)\n",
    "\n",
    "        # Create a bool mask for extracting the comparing points\n",
    "        self.compare_spot = compare_spot\n",
    "        self.compare_mask = torch.BoolTensor(*(self.target_image.size())).fill_(False).to(cuda)\n",
    "        for spot in compare_spot:\n",
    "            self.compare_mask[spot] = True\n",
    "\n",
    "        self.target_spot = self.target_image[self.compare_mask]\n",
    "\n",
    "        self.loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "        self.goal_reward = goal_reward\n",
    "        self.final_state_error_threshold = final_state_error_threshold\n",
    "        self.out_of_bound_penalty = out_of_bound_penalty\n",
    "\n",
    "\n",
    "    def __call__(self, result: FEMTaskResult) -> Tuple[Optional[float], bool]:\n",
    "        reward = 0.\n",
    "        final = False\n",
    "        \n",
    "        if result.state_fem_image is None:\n",
    "            return None, False\n",
    "        \n",
    "        state_spot = result.state_fem_image[0][self.compare_mask]\n",
    "        error = float(self.loss(state_spot, self.target_spot))\n",
    "        final = error <= self.final_state_error_threshold\n",
    "        \n",
    "        # If the state not changed(which means blocked by the environment),\n",
    "        # the out-of-bound penalty is applied\n",
    "        if torch.equal(result.state, result.next_state):\n",
    "            reward += self.out_of_bound_penalty\n",
    "\n",
    "        if result.next_state_fem_image is None:\n",
    "            return reward, final\n",
    "        \n",
    "        next_state_spot = result.next_state_fem_image[0][self.compare_mask]\n",
    "        next_error = float(self.loss(next_state_spot, self.target_spot))\n",
    "        \n",
    "        # Reward decreasing error from state to next state\n",
    "        reward += error - next_error\n",
    "        \n",
    "        # Reward extra points if next state is final\n",
    "        if next_error <= self.final_state_error_threshold:\n",
    "            reward += self.goal_reward\n",
    "            \n",
    "        log.step(result.episode, result.step)\n",
    "        log['rewards/reward'] = reward\n",
    "        log['rewards/error'] = error\n",
    "\n",
    "        return reward, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Network Container\n",
    "class Model():\n",
    "    def __init__(self, network: nn.Module, loss_func: _Loss, optimizer: Optimizer):\n",
    "        self.network = network\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __call__(self, network_input: Tensor) -> Tensor:\n",
    "        return self.network(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNet(state_size: int = 12, action_number: int = 24, target_network: bool = False):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(state_size, 100, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 200, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, action_number, device=cuda),\n",
    "    )\n",
    "    if target_network:\n",
    "        return Model(network=net, loss_func=None, optimizer=None)\n",
    "    else:\n",
    "        return Model(network=net, loss_func=nn.SmoothL1Loss(), optimizer=torch.optim.Adam(net.parameters(), 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory: deque = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment: GridHoleBoardEnv,\n",
    "                 fem_simulator: GridHoleBoardFEMSimulator,\n",
    "                 fem_reward_func: Callable[[Tensor, int, Tensor], float],\n",
    "                 q_network: nn.Module,\n",
    "                 target_network: nn.Module,\n",
    "                 target_update_interval: int = hp.target_update_interval,\n",
    "                 optimization_iter: int = hp.optimization_iter,\n",
    "                 max_step_per_episode: int = hp.max_step_per_episode,\n",
    "                 experience_replay: ReplayMemory = ReplayMemory(hp.experience_replay_size),\n",
    "                 replay_batch_size: int = hp.replay_batch_size,\n",
    "                 discount_factor: float = hp.discount_factor,\n",
    "                 explore_factor_start: float = hp.explore_factor_start,\n",
    "                 explore_factor_end: float = hp.explore_factor_end,\n",
    "                 explore_factor_decay: float = hp.explore_factor_decay,\n",
    "                ) -> None:\n",
    "        self.environment: GridHoleBoardEnv = environment\n",
    "        self.fem_simulator: GridHoleBoardFEMSimulator = fem_simulator\n",
    "        self.fem_reward_func: Callable[[Tensor, int, Tensor], float] = fem_reward_func\n",
    "\n",
    "        self.q_network: Model = q_network\n",
    "        self.target_network: Model = target_network\n",
    "        self.target_update_interval: int = target_update_interval\n",
    "\n",
    "        self.optimization_iter: int = optimization_iter\n",
    "        self.max_step_per_episode: int = max_step_per_episode\n",
    "        self.experience_replay: ReplayMemory = experience_replay\n",
    "        self.replay_batch_size: int = replay_batch_size\n",
    "\n",
    "        self.discount_factor: float = discount_factor\n",
    "        self.explore_factor_start: float = explore_factor_start\n",
    "        self.explore_factor_end: float = explore_factor_end\n",
    "        self.explore_factor_decay: float = explore_factor_decay\n",
    "        self.explored_step: int = 0\n",
    "\n",
    "\n",
    "    def select_action(self, disable_explore: bool = False) -> Tuple[State, int]:\n",
    "        state = self.environment.get_state()\n",
    "        explore_factor = self.explore_factor_end \\\n",
    "                            + (self.explore_factor_start - self.explore_factor_end) \\\n",
    "                            * math.exp(-1. * self.explored_step / self.explore_factor_decay)\n",
    "        exp['explore_factor'].log(explore_factor, step=self.explored_step)\n",
    "        if random.random() > explore_factor or disable_explore:\n",
    "            prediction = self.q_network(state.flatten())\n",
    "            action_index = prediction.argmax().item()\n",
    "        else:\n",
    "            action_index = random.randrange(len(self.environment.action_space))\n",
    "        self.explored_step += 1\n",
    "        return state, action_index\n",
    "\n",
    "    def step(self, episode: int, step: int, disable_explore: bool = False) -> bool:\n",
    "        state, action_index = self.select_action(disable_explore)\n",
    "\n",
    "        self.environment.step(self.environment.action_space[action_index])\n",
    "        next_state = self.environment.get_state()\n",
    "            \n",
    "        action = self.environment.action_space[action_index]\n",
    "        action_log = f'{action} ({state[action[0]]} => {next_state[action[0]]})'\n",
    "        exp['actions'].log(action_log)\n",
    "\n",
    "        self.fem_simulator.queue_transition(episode, step, state, action_index, next_state)\n",
    "\n",
    "        # Return True if the next state is final\n",
    "        return step >= self.max_step_per_episode\n",
    "\n",
    "    def complete_fem_rewarding(self, episode: int) -> None:\n",
    "        fem = self.fem_simulator    # Create local reference to shorter statements\n",
    "        print('Start computing rewards based on completed FEM simulation')\n",
    "        i = 0\n",
    "        \n",
    "        for result in fem.get_transition_result():\n",
    "            reward, final = self.fem_reward_func(result)\n",
    "            if reward is None:\n",
    "                continue\n",
    "\n",
    "            if final:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = result.next_state\n",
    "\n",
    "            self.experience_replay.push(result.state, result.action_index, reward, next_state)\n",
    "            \n",
    "            i += 1\n",
    "            if i % 100:\n",
    "                print(f'{i}/{self.max_step_per_episode}', end='\\r')\n",
    "            \n",
    "            if final:\n",
    "                fem.terminate_current_queue()\n",
    "                print('Early stop triggered')\n",
    "                return\n",
    "\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        if len(self.experience_replay) < self.replay_batch_size: return\n",
    "\n",
    "        samples = self.experience_replay.sample(self.replay_batch_size)\n",
    "        batch = Transition(*zip(*samples))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                  batch.next_state)), device=cuda, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s.flatten() for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "\n",
    "        state_batch = torch.stack([s.flatten() for s in batch.state])\n",
    "        action_batch = torch.tensor(batch.action_index, device=cuda).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(batch.reward, device=cuda)\n",
    "\n",
    "        state_action_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "        next_state_values = torch.zeros(self.replay_batch_size, device=cuda)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        loss = self.q_network.loss_func(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.q_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.q_network.network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.q_network.optimizer.step()\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        self.target_network.network.load_state_dict(self.q_network.network.state_dict())\n",
    "\n",
    "    def train(self, episode: int = hp.max_episode) -> None:\n",
    "        for e in range(episode):\n",
    "            start_time = datetime.now()\n",
    "            self.environment.reset()\n",
    "            # Use fixed number of steps in each episode\n",
    "            for step_num in range(self.max_step_per_episode):\n",
    "                self.step(e, step_num)\n",
    "            print(f'Episode {e} completed')\n",
    "            self.complete_fem_rewarding(e)\n",
    "            for i in range(self.optimization_iter):\n",
    "                self.optimize()\n",
    "            self.fem_simulator.clear_image_cache()\n",
    "            self.fem_simulator.reset_task_pool()\n",
    "            if e % self.target_update_interval == 0:\n",
    "                # print('Target network update                 ')\n",
    "                self.update_target_network()\n",
    "\n",
    "            end_time = datetime.now()\n",
    "            print(f'Episode {e} completed in {end_time - start_time}')\n",
    "\n",
    "    def generate(self, step: int = 100) -> State:\n",
    "        pass\n",
    "        # TODO: rewrite\n",
    "        # self.environment.reset()\n",
    "        # for i in range(step):\n",
    "        #     if self.step(100000000, i):\n",
    "        #         print(f'Completed in {step} steps                    ', end='\\r\\n')\n",
    "        #         break\n",
    "        # return self.environment.get_state(), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridHoleBoardEnv(size=ep.size, grid_size=ep.grid_size)\n",
    "\n",
    "fem = GridHoleBoardFEMSimulator(env)\n",
    "\n",
    "reward_func = FEMReward(env, 'target.png', [(73, 36),\n",
    "                                       (147, 36),\n",
    "                                       (73, 110),\n",
    "                                       (147, 110),\n",
    "                                       (73, 184),\n",
    "                                       (147, 184),\n",
    "                                       (73, 258),\n",
    "                                       (147, 258)])\n",
    "\n",
    "agent = Agent(env, fem, reward_func, QNet(), QNet(target_network=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 0 completed in 0:01:43.933093\n",
      "Episode 1 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 1 completed in 0:00:21.685933\n",
      "Episode 2 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 2 completed in 0:00:36.146539\n",
      "Episode 3 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 3 completed in 0:00:19.423703\n",
      "Episode 4 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 4 completed in 0:00:16.995210\n",
      "Episode 5 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 5 completed in 0:00:15.272616\n",
      "Episode 6 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 6 completed in 0:00:40.447813\n",
      "Episode 7 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 7 completed in 0:00:15.187500\n",
      "Episode 8 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 8 completed in 0:00:17.190133\n",
      "Episode 9 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 9 completed in 0:00:16.763961\n",
      "Episode 10 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 10 completed in 0:00:12.226949\n",
      "Episode 11 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 11 completed in 0:00:23.152624\n",
      "Episode 12 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 12 completed in 0:00:18.223918\n",
      "Episode 13 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 13 completed in 0:00:22.810516\n",
      "Episode 14 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 14 completed in 0:00:49.023859\n",
      "Episode 15 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 15 completed in 0:00:27.902308\n",
      "Episode 16 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 16 completed in 0:00:11.118040\n",
      "Episode 17 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 17 completed in 0:00:20.997629\n",
      "Episode 18 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 18 completed in 0:00:16.572763\n",
      "Episode 19 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 19 completed in 0:00:15.215243\n",
      "Episode 20 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 20 completed in 0:00:52.744885\n",
      "Episode 21 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 21 completed in 0:00:55.209135\n",
      "Episode 22 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 22 completed in 0:00:38.564130\n",
      "Episode 23 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 23 completed in 0:00:21.342266\n",
      "Episode 24 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 24 completed in 0:00:22.236278\n",
      "Episode 25 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 25 completed in 0:00:13.436734\n",
      "Episode 26 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 26 completed in 0:00:19.885178\n",
      "Episode 27 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 27 completed in 0:00:24.969964\n",
      "Episode 28 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 28 completed in 0:00:16.601562\n",
      "Episode 29 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 29 completed in 0:00:20.469977\n",
      "Episode 30 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 30 completed in 0:00:19.184707\n",
      "Episode 31 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 31 completed in 0:00:19.467409\n",
      "Episode 32 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 32 completed in 0:00:20.894954\n",
      "Episode 33 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 33 completed in 0:00:42.964765\n",
      "Episode 34 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 34 completed in 0:00:17.647751\n",
      "Episode 35 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 35 completed in 0:00:14.546322\n",
      "Episode 36 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 36 completed in 0:00:13.604187\n",
      "Episode 37 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 37 completed in 0:00:10.336517\n",
      "Episode 38 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 38 completed in 0:00:14.133346\n",
      "Episode 39 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 39 completed in 0:00:09.595349\n",
      "Episode 40 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 40 completed in 0:00:10.070045\n",
      "Episode 41 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 41 completed in 0:00:15.696973\n",
      "Episode 42 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 42 completed in 0:00:12.644659\n",
      "Episode 43 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "Episode 43 completed in 0:00:09.214657\n",
      "Episode 44 completed\n",
      "Start computing rewards based on completed FEM simulation\n",
      "2/1000\r"
     ]
    }
   ],
   "source": [
    "#print(agent.generate())\n",
    "\n",
    "agent.train(1000)\n",
    "\n",
    "#print(agent.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(list(range(len(agent.running_loss))), agent.running_loss, s=1, vmin=0, vmax=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = str(123)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([1, 2])\n",
    "torch.equal(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_stat: Dict[str, int] = {'solved': 0, 'image_cache': 0, 'file_cache': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8 MetaMaterial-RL",
   "language": "python",
   "name": "metamtl-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
