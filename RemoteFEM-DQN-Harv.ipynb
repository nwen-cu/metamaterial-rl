{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning- and FEM-based Inverse Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import neptune.new as neptune\n",
    "from neptune.new.types import File\n",
    "from neptune.new.utils import stringify_unsupported\n",
    "\n",
    "os.environ['NEPTUNE_PROJECT']=\"pil-clemson/metamtl-rl-test\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_ID']=\"45d03d69-6ac7-41ca-8af8-80caaa73aad5\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"metamaterial-rl/RemoteFEM-DQN.ipynb\"\n",
    "\n",
    "exp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_repeat = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Cloak', 'RandInit', 'Dev']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Union, Optional, Callable, Any\n",
    "from typing import Tuple, List, Set, Dict\n",
    "from typing import NamedTuple\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from types import SimpleNamespace\n",
    "import queue\n",
    "from queue import PriorityQueue\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pformat, pprint\n",
    "import multiprocessing\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimHubClient import SimHubClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Cores: 56\n"
     ]
    }
   ],
   "source": [
    "print('CPU Cores:', multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size: 376 GiB\n"
     ]
    }
   ],
   "source": [
    "# Getting all memory using os.popen()\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "mem_gib = mem_bytes/(1024.**3)\n",
    "print('Memory size:', int(mem_gib), 'GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: ['Tesla V100S-PCIE-32GB', 'Tesla V100S-PCIE-32GB']\n"
     ]
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print('GPUs:', available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current computing device: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cpu') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Current computing device:', cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DEBUG FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEBUG:\n",
    "    result_generation=True\n",
    "    result_visualization=False\n",
    "    \n",
    "    transition_log=True\n",
    "    transition_log_buffer=''\n",
    "    transition_log_buffer_gen=''\n",
    "\n",
    "    action_log=False\n",
    "    prediction_log=False\n",
    "    state_log=True\n",
    "    epsilon_log=True\n",
    "    reward_log=True\n",
    "    state_visualization=False\n",
    "    state_target_diff_visualization=False\n",
    "    \n",
    "    visualization_sampling_rate=.000\n",
    "    \n",
    "    optimizer_sample_log=False\n",
    "    \n",
    "    start_from_goal=False\n",
    "    \n",
    "    trace_memory=False\n",
    "    \n",
    "    in_generation_mode=False\n",
    "    \n",
    "    transition_checked=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG.trace_memory:\n",
    "    os.environ['PYTHONTRACEMALLOC'] = '3'\n",
    "    tracemalloc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = lambda x, l, u: l if x < l else u if x > u else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = {\n",
    "    'rings': 3,\n",
    "    \n",
    "    'result_size': (400, 400),\n",
    "    'result_range': (293.15, 353.15),\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'target_update_interval': 10,\n",
    "    'optimization_iterations': 1,\n",
    "    'experience_replay_capacity': 10000,\n",
    "    'replay_batch_size': 32,\n",
    "    'lr': .001,\n",
    "    'discount_factor': .9,\n",
    "    'epsilon_initial': 1.,\n",
    "    'epsilon_minimal': .1,\n",
    "    'epsilon_halflife': 2000,\n",
    "    'epsilon_boost_preterminal': .3,\n",
    "    'epsilon_generate': .1,\n",
    "\n",
    "    'max_episode': 300,\n",
    "    'max_step_per_episode': 500,\n",
    "    \n",
    "    'goal_reward': 10000.,\n",
    "    'terminal_threshold': [.1, .5],  #\n",
    "    'invalid_state_penalty': 0.,\n",
    "    'failed_episode_penalty': -10.,\n",
    "    \n",
    "    'low_value_earlier_stop_min_step': 150,\n",
    "    'low_value_earlier_stop_threshold': [0.3, 1.5],\n",
    "    'low_value_earlier_stop_step_count': 10,\n",
    "    \n",
    "    \n",
    "    'reward_func': ['-logx-0.5(norm,1.4,1.7)', '-logx'],  #\n",
    "    \n",
    "}    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaces and Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(dict):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def step(self, action: 'Action') -> 'State': \n",
    "        return action(copy.deepcopy(self))\n",
    "\n",
    "    def to_tensor(self) -> Tensor: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, name: str, action: Callable[[State], State]) -> None:\n",
    "        self.name = name\n",
    "        self.action = action\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return self.name\n",
    "        \n",
    "    def __call__(self, state: State) -> State:\n",
    "        return self.action(state)\n",
    "# Action = Callable[[State], State]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:    \n",
    "    def __init__(self) -> None:\n",
    "        self._state: State = None\n",
    "        self._action_space: List[Action] = list()\n",
    "        self._valid_actions: BoolTensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'''{self.__class__.__name__}(\n",
    "    Action space size: {self.action_count()}\n",
    "    Current state: {self.state}\n",
    ")'''\n",
    "        \n",
    "    @property\n",
    "    def state(self) -> State: return self._state\n",
    "    \n",
    "    @property\n",
    "    def action_space(self) -> List[Action]: return self._action_space        \n",
    "\n",
    "    def action_count(self) -> int: return len(self._action_space)        \n",
    "    \n",
    "    def reset(self) -> None: raise NotImplementedError\n",
    "        \n",
    "    def step(self, action_index: int) -> None: raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayTransition(NamedTuple):\n",
    "    state: State\n",
    "    action_index: int\n",
    "    reward: float\n",
    "    next_state: State\n",
    "    note: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimulationTransition:\n",
    "    # From stepping\n",
    "    episode: int\n",
    "    step: int\n",
    "    \n",
    "    state: State\n",
    "    action_index: int\n",
    "    next_state: State\n",
    "    \n",
    "    \n",
    "    action_name: str = None\n",
    "    action_type: str = None\n",
    "    \n",
    "    # From FEM simulator\n",
    "    state_id: str = None\n",
    "    next_state_id: str = None\n",
    "    \n",
    "    state_sim: Dict[str, Any] = None\n",
    "    next_state_sim: Dict[str, Any] = None\n",
    "    \n",
    "    # From reward function\n",
    "    state_sim_value: float = None\n",
    "    is_state_terminal: bool = None\n",
    "    state_terminal_type: int = None\n",
    "    \n",
    "    next_state_sim_value: float = None\n",
    "    is_next_state_terminal: bool = None\n",
    "    next_state_terminal_type: int = None\n",
    "    \n",
    "    reward: float = None    \n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'[{self.episode}-{self.step}] ' +\\\n",
    "                f'{self.state}({self.state_sim_value}, {self.is_state_terminal})' +\\\n",
    "                f' =={self.action_name}({self.action_type})==> ' +\\\n",
    "                f'{self.next_state}({self.state_sim_value}, {self.is_state_terminal})' +\\\n",
    "                f'  R:{self.reward} {\"Loop!\" if self.state_id == self.next_state_id else \"\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RewardFunc = Callable[[SimulationTransition], SimulationTransition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarvestRingState(State):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # # Fixed start\n",
    "        # self['r'] = [50, 60., 70., 80., 100.]\n",
    "        # self['k'] = [0., 30., 30., 30., 30., 10.]\n",
    "        # Random start\n",
    "        self['r'] = random.sample([float(x) for x in range(55, 100, 5)], k=3)\n",
    "        self['r'].append(50.)\n",
    "        self['r'].append(100.)\n",
    "        self['r'].sort()\n",
    "        \n",
    "        self['k'] = [0.]\n",
    "        for i in range(4):\n",
    "            self['k'].append(float(random.randint(0, 13) * 5))\n",
    "        self['k'].append(10.)\n",
    "    \n",
    "    def to_tensor(self) -> torch.Tensor:\n",
    "        return torch.cat([torch.tensor(self['r']), torch.tensor(self['k'])]).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 50.,  55.,  65.,  90., 100.,   0.,  30.,  25.,  35.,   5.,  10.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HarvestRingState().to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarvRingEnvironment(Environment):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rings = environment_config['rings']\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        def adjust_ring_r(ring: int, r_mod: float, lower_bound: float, upper_bound: float):\n",
    "            def action(state: State):\n",
    "                old_value = state['r'][ring]\n",
    "                new_value = clip(old_value + r_mod, lower_bound, upper_bound)\n",
    "                \n",
    "                if new_value == state['r'][ring - 1] or new_value == state['r'][ring + 1]:\n",
    "                    new_value = old_value\n",
    "                    \n",
    "                state['r'][ring] = new_value\n",
    "                return state\n",
    "            return Action(f'{ring}:r{r_mod}', action)\n",
    "                \n",
    "        def adjust_ring_k(ring, k_mod: float, lower_bound: float, upper_bound: float):\n",
    "            def action(state: State):\n",
    "                old_value = state['k'][ring]\n",
    "                new_value = clip(old_value + k_mod, lower_bound, upper_bound)\n",
    "                state['k'][ring] = new_value\n",
    "                return state\n",
    "            return Action(f'{ring}:k{k_mod}', action)\n",
    "          \n",
    "        for ring in [1, 2, 3]:\n",
    "            self._action_space.append(adjust_ring_r(ring, +5., 55., 95.))\n",
    "            self._action_space.append(adjust_ring_r(ring, -5., 55., 95.))\n",
    "        \n",
    "        for ring in [1, 2, 3, 4]:\n",
    "            self._action_space.append(adjust_ring_k(ring, +5., 0., 60.))\n",
    "            self._action_space.append(adjust_ring_k(ring, -5., 0., 60.))\n",
    "            \n",
    "        # TBD: adjust k of the board (region 3)\n",
    "            \n",
    "    def reset(self) -> None: \n",
    "        self._state = HarvestRingState()\n",
    "                \n",
    "        \n",
    "    def step(self, action_index: int) -> None: \n",
    "        action = self._action_space[action_index]\n",
    "        self._state = self._state.step(action)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = HarvRingEnvironment()\n",
    "\n",
    "\n",
    "env.step(0)\n",
    "print(env._state)\n",
    "print(id(env._state))\n",
    "\n",
    "env.reset()\n",
    "print(env._state)\n",
    "print(id(env._state))\n",
    "\n",
    "env._action_space\n",
    "\n",
    "\n",
    "env.step(4)\n",
    "print(env._state)\n",
    "print(id(env._state))\n",
    "\n",
    "clip(env._state['r'][1] + 5, 55, 95)\n",
    "\n",
    "env._state['r'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEM-based Reward & Terminal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badloe(reward_thresholds: List[float]):\n",
    "    low, mid, high = reward_thresholds\n",
    "    def func(eta):\n",
    "        if eta < low:\n",
    "            return -10\n",
    "        if eta > high:\n",
    "            return 10000\n",
    "        return (eta / mid) ** 9 - 1 \n",
    "    return func\n",
    "\n",
    "reward_funcs = {\n",
    "    'linear': lambda x: x,\n",
    "    '-logx': lambda x: -np.log(x),\n",
    "    '-logx-0.5': lambda x: -np.log(x) - 0.5,\n",
    "    '-logx-0.5(norm,1.4,1.7)': lambda x: (-np.log(x) + 0.9) / 1.7 - 1,\n",
    "    # 'badloe': badloe(hyperparameters['reward_thresholds']),\n",
    "}\n",
    "\n",
    "class FEMReward():\n",
    "    def __init__(self,\n",
    "                 hyperparameters: Dict[str, Any]) -> None:\n",
    "        \n",
    "        self.reward_func = [reward_funcs[func] for func in hyperparameters['reward_func']]\n",
    "        \n",
    "        self.max_step_per_episode = hyperparameters['max_step_per_episode']\n",
    "\n",
    "        self.goal_reward = hyperparameters['goal_reward']\n",
    "        self.terminal_threshold = hyperparameters['terminal_threshold']\n",
    "        self.invalid_state_penalty = hyperparameters['invalid_state_penalty']\n",
    "        self.failed_episode_penalty = hyperparameters['failed_episode_penalty']\n",
    "        \n",
    "        self.current_episode = 0\n",
    "        \n",
    "        self.low_value_min_step = hyperparameters['low_value_earlier_stop_min_step']\n",
    "        self.low_value_threshold = hyperparameters['low_value_earlier_stop_threshold']\n",
    "        self.low_value_step_threshold = hyperparameters['low_value_earlier_stop_step_count']\n",
    "        self.low_value_step_count = 0\n",
    "        \n",
    "\n",
    "    def __call__(self, transition: SimulationTransition) -> SimulationTransition:\n",
    "        \"\"\"\n",
    "        Calculate reward value for a transition, and determine if a terminal state is reached\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition : SimulationTransition\n",
    "            A transition with completed simulation data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float | None\n",
    "            Reward value, None if the next_state is terminal\n",
    "        bool\n",
    "            The next_state is terminal\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        \n",
    "        if transition.episode != self.current_episode:\n",
    "            self.low_value_step_count = 0\n",
    "            self.current_episode = transition.episode\n",
    "        \n",
    "\n",
    "        size = environment_config['result_size']\n",
    "        center = (int(size[0] / 2), int(size[1] / 2))\n",
    "        cloaked_radius = int(transition.state['r'][0])\n",
    "        t_delta_ref = .3 * cloaked_radius\n",
    "        \n",
    "        \n",
    "        Y, X = np.ogrid[:size[0], :size[1]]\n",
    "        dist_from_center = np.sqrt((X - center[0]) ** 2 + (Y - center[1]) ** 2)\n",
    "        t_dist_mask = dist_from_center <= cloaked_radius\n",
    "        t_dist_ref = np.load('ref400.npy')\n",
    "        \n",
    "        if transition.state_sim and transition.state_sim['status'] == 'done':\n",
    "            state_t_dist = transition.state_sim['output']['temperature_distribution'][2].reshape(environment_config['result_size'])\n",
    "            state_t_a = state_t_dist[center[0], center[1] - cloaked_radius]\n",
    "            state_t_b = state_t_dist[center[0], center[1] + cloaked_radius]\n",
    "            \n",
    "            state_t_delta = np.abs(state_t_a - state_t_b)\n",
    "            \n",
    "            state_t_neuturality = np.sum(np.abs(state_t_dist - t_dist_ref)) / t_dist_mask.size\n",
    "            \n",
    "            transition.state_sim_value = [state_t_delta / t_delta_ref, state_t_neuturality]\n",
    "            transition.is_state_terminal = transition.state_sim_value[0] <= self.terminal_threshold[0] \\\n",
    "                and transition.state_sim_value[1] <= self.terminal_threshold[1]\n",
    "\n",
    "        \n",
    "        if transition.next_state_sim and transition.next_state_sim['status'] == 'done':\n",
    "            next_state_t_dist = transition.next_state_sim['output']['temperature_distribution'][2].reshape(environment_config['result_size'])\n",
    "            next_state_t_a = next_state_t_dist[center[0], center[1] - cloaked_radius]\n",
    "            next_state_t_b = next_state_t_dist[center[0], center[1] + cloaked_radius]\n",
    "            \n",
    "            next_state_t_delta = np.abs(next_state_t_a - next_state_t_b)\n",
    "            \n",
    "            next_state_t_neuturality = np.sum(np.abs(next_state_t_dist - t_dist_ref)) / t_dist_mask.size\n",
    "            \n",
    "            transition.next_state_sim_value = [next_state_t_delta / t_delta_ref, next_state_t_neuturality]\n",
    "            transition.is_next_state_terminal = transition.next_state_sim_value <= self.terminal_threshold \\\n",
    "                and transition.state_sim_value[1] <= self.terminal_threshold[1]\n",
    "        \n",
    "        if transition.state_sim_value:\n",
    "            transition.reward = self.reward_func[0](transition.state_sim_value[0]) \\\n",
    "                + self.reward_func[1](transition.state_sim_value[1])\n",
    "            exp['reward_dT'].append(self.reward_func[0](transition.state_sim_value[0]))\n",
    "            exp['reward_Mv'].append(self.reward_func[1](transition.state_sim_value[1]))\n",
    "            exp[f'reward_total'].append(transition.reward)\n",
    "            exp[f'reward_total_episode/{transition.episode}'].append(transition.reward)\n",
    "        else:\n",
    "            transition.reward = self.invalid_state_penalty\n",
    "        \n",
    "        # Reward only for primary terminal condition\n",
    "        if transition.is_state_terminal:\n",
    "            transition.reward = self.goal_reward\n",
    "            transition.state_terminal_type = 0  # 0 for success episode\n",
    "        else:\n",
    "            # Determine alternative terminal\n",
    "            # Alt-term 1 (after X step)\n",
    "            if transition.step >= self.max_step_per_episode - 1:\n",
    "                transition.is_state_terminal = True;\n",
    "                transition.state_terminal_type = 1\n",
    "                transition.reward = self.failed_episode_penalty\n",
    "            # Alt-term 2 (trapped in low-val area)\n",
    "            elif transition.step >= self.low_value_min_step: \n",
    "                if transition.state_sim_value[0] > self.low_value_threshold[0] \\\n",
    "                    and transition.state_sim_value[1] > self.low_value_threshold[1]:\n",
    "                    self.low_value_step_count += 1\n",
    "                    exp['low_val'].append(f'E{transition.episode} Low value step {self.low_value_step_count} {transition.state_sim_value}')\n",
    "                    if self.low_value_step_count >= self.low_value_step_threshold:\n",
    "                        transition.is_state_terminal = True;\n",
    "                        transition.state_terminal_type = 2\n",
    "                        transition.reward = self.failed_episode_penalty\n",
    "                        exp['low_val'].append(f'E{transition.episode} Low value terminal {self.low_value_step_count}')\n",
    "                else:\n",
    "                    if self.low_value_step_count > 0:\n",
    "                        exp['low_val'].append(f'E{transition.episode} Low value count reset')\n",
    "                    self.low_value_step_count = 0\n",
    "\n",
    "            \n",
    "        # LOGGING\n",
    "        if transition.state_sim and transition.state_sim['status'] == 'done':\n",
    "            if DEBUG.state_log:\n",
    "                if not DEBUG.in_generation_mode:\n",
    "                    exp['state_values_dT'].append(transition.state_sim_value[0])\n",
    "                    exp['state_values_Mv'].append(transition.state_sim_value[1])\n",
    "                else:\n",
    "                    exp['state_values_gen_dT'].append(transition.state_sim_value[0])  \n",
    "                    exp['state_values_gen_Mv'].append(transition.state_sim_value[1])            \n",
    "                    \n",
    "        if DEBUG.reward_log and not DEBUG.in_generation_mode:                \n",
    "            exp['reward'].append(transition.reward)\n",
    "        \n",
    "        return transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Network Container\n",
    "class Model():\n",
    "    def __init__(self, network: nn.Module, loss_func: _Loss, optimizer: Optimizer):\n",
    "        self.network = network\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __call__(self, network_input: Tensor) -> Tensor:\n",
    "        return self.network(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNet(state_size: int = 11, action_number: int = 14, target_network: bool = False):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(state_size, 128, device=cuda, dtype=torch.double),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256, device=cuda, dtype=torch.double),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, action_number, device=cuda, dtype=torch.double),\n",
    "    )\n",
    "    if target_network:\n",
    "        return Model(network=net, loss_func=None, optimizer=None)\n",
    "    else:\n",
    "        # exp['Network'] = str(torchinfo.summary(net, input_size=(32, state_size), \n",
    "        #                                        device=cuda, verbose=0))\n",
    "        return Model(network=net, loss_func=nn.HuberLoss(), optimizer=torch.optim.Adam(net.parameters(), 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory: deque = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(ReplayTransition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, environment: Environment, simulator: SimHubClient, reward_func: RewardFunc, \n",
    "                 policy_network: nn.Module, target_network: nn.Module, hyperparameters: Dict[str, Any]) -> None:\n",
    "        self.environment: Environment = environment\n",
    "        self.fem_simulator: SimHubClient = simulator\n",
    "        self.fem_reward_func: RewardFunc = reward_func\n",
    "        \n",
    "        self.policy_network: Model = policy_network\n",
    "        self.target_network: Model = target_network  \n",
    "        \n",
    "        self.target_update_interval: int = hyperparameters['target_update_interval']\n",
    "\n",
    "        self.optimization_iterations: int = hyperparameters['optimization_iterations']\n",
    "        self.max_step_per_episode: int = hyperparameters['max_step_per_episode']\n",
    "        self.experience_replay: ReplayMemory = ReplayMemory(hyperparameters['experience_replay_capacity'])\n",
    "        self.replay_batch_size: int = hyperparameters['replay_batch_size']\n",
    "\n",
    "        self.discount_factor: float = hyperparameters['discount_factor']\n",
    "        self.epsilon_initial: float = hyperparameters['epsilon_initial']\n",
    "        self.epsilon_minimal: float = hyperparameters['epsilon_minimal']\n",
    "        self.epsilon_halflife: float = hyperparameters['epsilon_halflife']\n",
    "        self.epsilon_generate: float = hyperparameters['epsilon_generate']\n",
    "        \n",
    "        self.epsilon_boost_preterminal: float = hyperparameters['epsilon_boost_preterminal']\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pending_transitions: List[SimulationTransition] = list()\n",
    "        \n",
    "        # Set to true when generating result\n",
    "        self.generation_mode: bool = False\n",
    "        self.explored_step: int = 0\n",
    "        \n",
    "        self.total_steps: int = 0\n",
    "        \n",
    "        self.episode: int = 0\n",
    "        self.step_num: int = 0\n",
    "        \n",
    "        # Logging result of episode and boost epsilon when needed\n",
    "        self.previous_episode_terminal: List[bool] = list()\n",
    "        self.terminal_reached: bool = False\n",
    "        \n",
    "        self.convergence_episode: int = 0\n",
    "        self.convergence_step: int = 100000\n",
    "        self.convergence_episode_gen: int = 0\n",
    "        self.convergence_step_gen: int = 100000\n",
    "\n",
    "        \n",
    "    def updaet_action_mask(self) -> None:\n",
    "        ...\n",
    "        \n",
    "    \n",
    "    def select_action(self) -> Tuple[State, int, str]:\n",
    "        \"\"\"\n",
    "        Decide an action based on epsilon greedy algorithm\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        State\n",
    "            Current state instance\n",
    "        int\n",
    "            Index number of an action in the action space\n",
    "        \n",
    "        str\n",
    "            Action type, literal string of \"Prediction\" or \"Random\"\n",
    "        \"\"\"\n",
    "        state = self.environment.state\n",
    "        # epsilon = self.epsilon_minimal + (self.epsilon_initial  - self.epsilon_minimal) * \\\n",
    "        #             math.exp(-1. * self.total_steps / self.epsilon_decay)\n",
    "        \n",
    "        # Determining epsilon\n",
    "        \n",
    "        epsilon = max(self.epsilon_initial \n",
    "                             * (0.5 ** (self.total_steps / self.epsilon_halflife)), \n",
    "                             self.epsilon_minimal)\n",
    "        \n",
    "        if not self.terminal_reached: epsilon += self.epsilon_boost_preterminal\n",
    "        \n",
    "        if self.generation_mode: epsilon = self.epsilon_generate\n",
    "        \n",
    "        # Epsilon determined\n",
    "        \n",
    "        if DEBUG.epsilon_log:\n",
    "            if self.generation_mode:\n",
    "                exp['epsilon_gen'].append(epsilon, step=self.episode + self.step_num / self.max_step_per_episode)\n",
    "            else:\n",
    "                exp['epsilon'].append(epsilon, step=self.episode + self.step_num / self.max_step_per_episode)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            prediction = self.policy_network(state.to_tensor().flatten()).flatten()\n",
    "            \n",
    "            if DEBUG.prediction_log:\n",
    "                if not self.generation_mode:\n",
    "                    log_target = 'prediction'\n",
    "                else:\n",
    "                    log_target = 'prediction_gen'\n",
    "                exp[f'{log_target}/{self.episode}'].append(f'Step {self.step_num}', step=self.step_num)\n",
    "                exp[f'{log_target}/{self.episode}'].append(str(state), step=self.step_num+0.1)\n",
    "                preds = []\n",
    "                for i in range(len(prediction)):\n",
    "                    preds.append((prediction[i].item(), str(self.environment.action_space[i])))\n",
    "                preds.sort(reverse=True)\n",
    "                exp[f'{log_target}/{self.episode}'].append(pformat(preds), step=self.step_num+0.2)\n",
    "                \n",
    "            action_index = prediction.argmax().item()\n",
    "            action_type = 'Prediction'\n",
    "            \n",
    "        else:\n",
    "            action_index = random.randrange(len(self.environment.action_space))\n",
    "            action_type = 'Random'\n",
    "            self.explored_step += 1\n",
    "            \n",
    "        if not self.generation_mode:\n",
    "            self.total_steps += 1\n",
    "        return state, action_index, action_type\n",
    "    \n",
    "    def step(self) -> SimulationTransition:\n",
    "        \"\"\"\n",
    "        Perform an action in the in the environment and submit the transition as FEM task to simulator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        SimulationTransition\n",
    "            Return the transition\n",
    "        \"\"\"\n",
    "        state, action_index, action_type = self.select_action()\n",
    "        self.environment.step(action_index)\n",
    "        next_state = self.environment.state\n",
    "        \n",
    "        \n",
    "        transition = SimulationTransition(self.episode, self.step_num, state, action_index, next_state)\n",
    "        transition.state_id, state_result = self.fem_simulator.submit_task(state)\n",
    "        transition.next_state_id, next_state_result = self.fem_simulator.submit_task(next_state)\n",
    "        transition.action_type = action_type\n",
    "        transition.action_name = self.environment.action_space[action_index].name\n",
    "        \n",
    "        self.pending_transitions.append(transition)\n",
    "        return transition\n",
    "    \n",
    "    def compute_reward(self, transition: SimulationTransition) -> None:\n",
    "        \"\"\"\n",
    "        Compute reward value and terminal status for a COMPLETED transition. \n",
    "        The states, action and reward will be pushed into experience replay\n",
    "        \n",
    "        If the current state is terminal, transition.next_state will be set to None\n",
    "\n",
    "        \"\"\"\n",
    "        self.fem_reward_func(transition)\n",
    "        \n",
    "        if DEBUG.transition_log:\n",
    "            if not self.generation_mode:\n",
    "                DEBUG.transition_log_buffer += str(transition) + '\\n'\n",
    "                # exp[f'transitions/{transition.episode}'].append(str(transition))\n",
    "            else:\n",
    "                DEBUG.transition_log_buffer_gen += str(transition) + '\\n'\n",
    "                # exp[f'transitions_gen/{transition.episode}'].append(str(transition))\n",
    "\n",
    "        if not self.generation_mode:\n",
    "            self.experience_replay.push(transition.state.to_tensor(), \n",
    "                                        transition.action_index, \n",
    "                                        transition.reward, \n",
    "                                        None if transition.is_state_terminal else transition.next_state.to_tensor(),\n",
    "                                        f'{transition.episode}-{transition.step}')\n",
    "\n",
    "        \n",
    "    def compute_pending_rewards(self) -> Tuple[SimulationTransition, float]:\n",
    "        episode_return: float = 0.\n",
    "        for transition in tqdm(self.pending_transitions):\n",
    "            terminal_transition = transition\n",
    "            \n",
    "            transition.state_sim = self.fem_simulator.wait_for_task(transition.state_id)\n",
    "            transition.next_state_sim = self.fem_simulator.wait_for_task(transition.next_state_id)\n",
    "        \n",
    "            self.compute_reward(transition)\n",
    "            \n",
    "            episode_return = episode_return * self.discount_factor + transition.reward;\n",
    "            \n",
    "            if transition.is_state_terminal:\n",
    "                self.terminal_reached = True\n",
    "                if not self.generation_mode:\n",
    "                    exp['terminal_type'].append(transition.state_terminal_type, step=transition.episode)\n",
    "                else:\n",
    "                    exp['terminal_type_gen'].append(transition.state_terminal_type, step=transition.episode)\n",
    "                if transition.step < self.max_step_per_episode - 1: \n",
    "                    break\n",
    "\n",
    "                \n",
    "        self.pending_transitions.clear()\n",
    "        return transition, episode_return\n",
    "        \n",
    "    def optimize(self) -> None:\n",
    "        if len(self.experience_replay) < self.replay_batch_size: return\n",
    "\n",
    "        for i in range(self.optimization_iterations):\n",
    "            samples = self.experience_replay.sample(self.replay_batch_size)\n",
    "            batch = ReplayTransition(*zip(*samples))\n",
    "            \n",
    "            if DEBUG.optimizer_sample_log:\n",
    "                filename = f'logs/sampled_transition-{self.total_steps + i / self.optimization_iterations}.log'\n",
    "                with open(filename, 'w') as fp:\n",
    "                    pprint(samples, stream=fp)\n",
    "                exp['sampled_transition'].upload_files(filename)\n",
    "\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                      batch.next_state)), device=cuda, dtype=torch.bool)\n",
    "            # If none of the transition has a valid next_step, skip the round\n",
    "            if not non_final_mask.any():\n",
    "                return\n",
    "            non_final_next_states = torch.stack([s.flatten() for s in batch.next_state\n",
    "                                                            if s is not None])\n",
    "\n",
    "            state_batch = torch.stack([s.flatten() for s in batch.state])\n",
    "            action_batch = torch.tensor(batch.action_index, device=cuda).unsqueeze(1)\n",
    "            reward_batch = torch.tensor(batch.reward, device=cuda)\n",
    "\n",
    "            state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "            next_state_values = torch.zeros(self.replay_batch_size, device=cuda)\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "            expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "            loss = self.policy_network.loss_func(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            optimization_loss = float(loss)\n",
    "            self.policy_network.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_network.network.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.policy_network.optimizer.step()\n",
    "            \n",
    "            exp['optimization_loss'].append(optimization_loss, step=self.total_steps + i / self.optimization_iterations)\n",
    "        \n",
    "    def update_target_network(self) -> None:\n",
    "        self.target_network.network.load_state_dict(self.policy_network.network.state_dict())\n",
    "        \n",
    "    def train(self, episodes: int) -> None:\n",
    "        for episode in range(episodes):\n",
    "            print('')\n",
    "            print(f'Episode: {episode}')\n",
    "            self.episode = episode\n",
    "            \n",
    "            self.environment.reset()\n",
    "            \n",
    "            print('Stepping...')\n",
    "            for self.step_num in trange(self.max_step_per_episode):\n",
    "                self.step()\n",
    "\n",
    "                self.optimize()\n",
    "                \n",
    "                if self.total_steps % self.target_update_interval == 0:\n",
    "                    self.update_target_network()\n",
    "                    \n",
    "            exp['total_explored'].append(self.explored_step, step=self.episode)\n",
    "\n",
    "            print('Processing rewards...')\n",
    "            transition, episode_return = self.compute_pending_rewards()\n",
    "            \n",
    "            if transition.is_state_terminal and transition.state_terminal_type == 0: \n",
    "                print(f'Terminal state found in episode {transition.episode} step {transition.step} deltaT {transition.state_sim_value}:')\n",
    "                print(transition.state)\n",
    "                exp['goal_reached'].append(f'{transition.state}-{transition.state_sim_value}', step=self.episode)\n",
    "           \n",
    "            print(f'Episode return: {episode_return}')\n",
    "            exp['episode_return'].append(episode_return, step=self.episode)\n",
    "                \n",
    "            exp['terminal_step'].append(transition.step, step=self.episode)\n",
    "            \n",
    "            if transition.step < self.convergence_step:\n",
    "                self.convergence_step = transition.step\n",
    "                self.convergence_episode = transition.episode\n",
    "            elif transition.step > self.convergence_step:\n",
    "                self.convergence_step = 100000\n",
    "                self.convergence_episode = 0                \n",
    "            \n",
    "            self.fem_simulator.clear_tasks()\n",
    "\n",
    "            if DEBUG.result_generation:\n",
    "                generated_result = self.generate()\n",
    "                \n",
    "                exp['generated_result'].append(str(generated_result), step=self.episode)\n",
    "                \n",
    "                if DEBUG.result_visualization:\n",
    "                    print('Visualizing result...')\n",
    "                    log_vis_sim(generated_result.state_sim['output']['temperature_distribution'][2], 'generated_state_vis', \n",
    "                                append=True, step=self.episode, vrange=(293.15, 353.15))\n",
    "                \n",
    "            if DEBUG.transition_log:\n",
    "                log_file = f'logs/transition-{self.episode}.log'\n",
    "                with open(log_file, 'w') as fp:\n",
    "                    fp.write(DEBUG.transition_log_buffer)\n",
    "                DEBUG.transition_log_buffer = ''\n",
    "                exp['transition_log'].upload_files(log_file)\n",
    "                \n",
    "                log_file = f'logs/transition-gen-{self.episode}.log'\n",
    "                with open(log_file, 'w') as fp:\n",
    "                    fp.write(DEBUG.transition_log_buffer_gen)\n",
    "                DEBUG.transition_log_buffer_gen = ''\n",
    "                exp['transition_log'].upload_files(log_file)\n",
    "\n",
    "            \n",
    "            if DEBUG.trace_memory:\n",
    "                snapshot = tracemalloc.take_snapshot()\n",
    "                with open(f'logs/mem{self.episode}.log', 'w') as fp:\n",
    "                    for line in snapshot.statistics('lineno')[:30]:\n",
    "                        print(line, file=fp)\n",
    "                        \n",
    "            torch.save(self.policy_network, f'checkpoints/checkpoint_{self.episode}.pt')\n",
    "            exp[f'model_checkpoints/policy_network/{self.episode}'].upload(f'checkpoints/checkpoint_{self.episode}.pt')\n",
    "            \n",
    "        \n",
    "    def generate(self) -> State:\n",
    "        self.generation_mode = True\n",
    "        if DEBUG:\n",
    "            DEBUG.in_generation_mode=True\n",
    "            \n",
    "        print('Generating...')\n",
    "            \n",
    "        self.environment.reset()\n",
    "            \n",
    "        for self.step_num in trange(self.max_step_per_episode):\n",
    "            self.step()\n",
    "\n",
    "            \n",
    "        print('Evaluating states...')\n",
    "        transition, episode_return = self.compute_pending_rewards()\n",
    "        \n",
    "        if transition.is_state_terminal and transition.state_terminal_type == 0: \n",
    "            print(f'Terminal state reached in step {transition.step} deltaT {transition.state_sim_value}:')\n",
    "            print(transition.state)\n",
    "            exp['goal_reached_gen'].append(f'{transition.state}-{transition.state_sim_value}', step=self.episode) \n",
    "        \n",
    "        print(f'Episode return: {episode_return}')\n",
    "        exp['episode_return_gen'].append(episode_return, step=self.episode)\n",
    "            \n",
    "        exp['terminal_step_gen'].append(transition.step, step=self.episode)\n",
    "        \n",
    "        if transition.step < self.convergence_step_gen:\n",
    "            self.convergence_step_gen = transition.step\n",
    "            self.convergence_episode_gen = transition.episode\n",
    "        elif transition.step > self.convergence_step:\n",
    "            self.convergence_step_gen = 100000\n",
    "            self.convergence_episode_gen = 0         \n",
    "        \n",
    "        self.generation_mode = False\n",
    "        if DEBUG:\n",
    "            DEBUG.in_generation_mode=False\n",
    "        return transition\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pil-clemson/metamtl-rl-test/e/RLTEST-95\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Waiting for database connection..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.1294431.pbs02/ipykernel_575597/1842248645.py:17: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  exp['EnvConfig'] = stringify_unsupported(environment_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database\n",
      "\n",
      "Episode: 0\n",
      "Stepping...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013623237609863281,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 500,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35e026043cd421f9c2d26d9048f84e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rewards...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013081073760986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 500,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1fad73432547cbb1bcacdf95275356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(experiment_repeat):\n",
    "    #\n",
    "    exp = neptune.init_run(project=\"pil-clemson/metamtl-rl-test\",\n",
    "                           capture_hardware_metrics=True,\n",
    "                           capture_stderr=True,\n",
    "                           capture_stdout=True,\n",
    "                           source_files=['RemoteFEM-DQN-Harv.ipynb'],\n",
    "                          )\n",
    "    \n",
    "    exp['sys/tags'].add(tags)\n",
    "    \n",
    "    #\n",
    "    if i > 0:\n",
    "        exp['sys/tags'].add(['Rerun'])\n",
    "\n",
    "    #\n",
    "    exp['EnvConfig'] = stringify_unsupported(environment_config)\n",
    "    exp['Hyperparameters'] = stringify_unsupported(hyperparameters)\n",
    "\n",
    "    #\n",
    "    env = HarvRingEnvironment()\n",
    "\n",
    "    fem = SimHubClient('10.128.97.115')\n",
    "    fem.set_experiment('./elmer_thermal_cloak_ring/elmer_task.yml')\n",
    "\n",
    "    reward_func = FEMReward(hyperparameters)\n",
    "\n",
    "    agent = Agent(env, fem, reward_func, QNet(), QNet(target_network=True), hyperparameters)\n",
    "\n",
    "    #\n",
    "    agent.train(hyperparameters['max_episode'])\n",
    "\n",
    "    #\n",
    "    exp['convergence'] = f'{agent.convergence_episode}({agent.convergence_step})' \\\n",
    "                        + f'/{agent.convergence_episode_gen}({agent.convergence_step_gen})'\n",
    "\n",
    "    #\n",
    "    agent.episode += 1\n",
    "    generated_result = agent.generate()\n",
    "\n",
    "    exp['generated_result_final'] = str(generated_result)\n",
    "\n",
    "    exp['sys/tags'].add(['Done'])\n",
    "    if generated_result.state_sim_value <= hyperparameters['terminal_threshold']:\n",
    "        exp['sys/tags'].add(['Sucessful'])\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "    #\n",
    "    fem.close()\n",
    "\n",
    "    #\n",
    "    exp.stop()\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRL3.10",
   "language": "python",
   "name": "pyrl3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "neptune": {
   "notebookId": "45d03d69-6ac7-41ca-8af8-80caaa73aad5",
   "projectVersion": 2
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
