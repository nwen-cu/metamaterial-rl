{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning- and FEM-based Inverse Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pil-clemson/metamtl-rl/e/METAMTLRL-168\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import neptune.new as neptune\n",
    "\n",
    "os.environ['NEPTUNE_PROJECT']=\"pil-clemson/metamtl-rl\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_ID']=\"45d03d69-6ac7-41ca-8af8-80caaa73aad5\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"metamaterial-rl/RemoteFEM-DQN.ipynb\"\n",
    "\n",
    "exp = neptune.init_run(project=\"pil-clemson/metamtl-rl\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Union, Optional, Callable, Any\n",
    "from typing import Tuple, List, Set, Dict\n",
    "from typing import NamedTuple\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from types import SimpleNamespace\n",
    "import queue\n",
    "from queue import PriorityQueue\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pformat\n",
    "import multiprocessing\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimHubClient import SimHubClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Cores: 56\n"
     ]
    }
   ],
   "source": [
    "print('CPU Cores:', multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size: 376 GiB\n"
     ]
    }
   ],
   "source": [
    "# Getting all memory using os.popen()\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "mem_gib = mem_bytes/(1024.**3)\n",
    "print('Memory size:', int(mem_gib), 'GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: ['Tesla V100S-PCIE-32GB', 'Tesla V100S-PCIE-32GB']\n"
     ]
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print('GPUs:', available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current computing device: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cpu') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Current computing device:', cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = lambda x, l, u: l if x < l else u if x > u else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sim():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.105119.pbs02/ipykernel_3181230/3574678125.py:30: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  exp['EnvConfig'] = environment_configuration\n"
     ]
    }
   ],
   "source": [
    "environment_configuration = {\n",
    "    'grid_size': (4, 4),\n",
    "    'result_size': (40, 40),\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'target_update_interval': 100,\n",
    "    'optimization_iterations': 10,\n",
    "    'experience_replay_capacity': 10000,\n",
    "    'replay_batch_size': 32,\n",
    "    'lr': .001,\n",
    "    'discount_factor': .9,\n",
    "    'explore_factor_initial': 1.,\n",
    "    'explore_factor_minimal': 0.05,\n",
    "    'explore_factor_halflife': 2000.,\n",
    "\n",
    "    'final_threshold': 0.01,\n",
    "    'final_step_threshold': 100,\n",
    "\n",
    "    'max_episode': 1000,\n",
    "    'max_step_per_episode': 1000,\n",
    "    \n",
    "    'goal_reward':10000.,\n",
    "    'terminal_error_threshold': 100.,\n",
    "    'invalid_state_penalty': -1000000.,\n",
    "    'loop_penalty':-1000000.,\n",
    "    \n",
    "}    \n",
    "\n",
    "exp['EnvConfig'] = environment_configuration\n",
    "exp['Hyperparameters'] = hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaces and Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(dict):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def step(self, action: 'Action') -> 'State': \n",
    "        return action(copy.deepcopy(self))\n",
    "\n",
    "    def to_tensor(self) -> Tensor: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, name: str, action: Callable[[State], State]) -> None:\n",
    "        self.name = name\n",
    "        self.action = action\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return self.name\n",
    "        \n",
    "    def __call__(self, state: State) -> State:\n",
    "        return self.action(state)\n",
    "# Action = Callable[[State], State]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:    \n",
    "    def __init__(self) -> None:\n",
    "        self._state: State = None\n",
    "        self._action_space: List[Action] = list()\n",
    "        self._valid_actions: BoolTensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'''{self.__class__.__name__}(\n",
    "    Action space size: {self.action_count()}\n",
    "    Current state: {self.state}\n",
    ")'''\n",
    "        \n",
    "    @property\n",
    "    def state(self) -> State: return self._state\n",
    "    \n",
    "    @property\n",
    "    def action_space(self) -> List[Action]: return self._action_space        \n",
    "\n",
    "    def action_count(self) -> int: return len(self._action_space)        \n",
    "\n",
    "    @property\n",
    "    def valid_actions(self) -> BoolTensor: return self._valid_actions\n",
    "    \n",
    "    def reset(self) -> None: raise NotImplementedError\n",
    "        \n",
    "    def step(self, action_index: int) -> None: raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayTransition(NamedTuple):\n",
    "    state: State\n",
    "    action_index: int\n",
    "    reward: float\n",
    "    next_state: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimulationTransition:\n",
    "    # From stepping\n",
    "    episode: int\n",
    "    step: int\n",
    "    \n",
    "    state: State\n",
    "    action_index: int\n",
    "    next_state: State\n",
    "    \n",
    "    \n",
    "    action_name: str = None\n",
    "    \n",
    "    # From FEM simulator\n",
    "    state_id: str = None\n",
    "    next_state_id: str = None\n",
    "    \n",
    "    state_sim: Dict[str, Any] = None\n",
    "    next_state_sim: Dict[str, Any] = None\n",
    "    \n",
    "    # From reward function\n",
    "    state_error: float = None\n",
    "    is_state_terminal: bool = None\n",
    "    \n",
    "    next_state_error: float = None\n",
    "    is_next_state_terminal: bool = None\n",
    "    \n",
    "    reward: float = None    \n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'[{self.episode}-{self.step}] ' +\\\n",
    "                f'{self.state}({self.state_error}, {self.is_state_terminal})' +\\\n",
    "                f' =={self.action_name}==> ' +\\\n",
    "                f'{self.next_state}({self.state_error}, {self.is_state_terminal})' +\\\n",
    "                f'  R:{self.reward} {\"Loop!\" if self.state_id == self.next_state_id else \"\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RewardFunc = Callable[[SimulationTransition], SimulationTransition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnableGridState(State):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self['angle_matrix'] = np.zeros(environment_configuration['grid_size'])\n",
    "    \n",
    "    def to_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self['angle_matrix'].flatten()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnableGridEnvironment(Environment):    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.grid_size = environment_configuration['grid_size']\n",
    "        \n",
    "        self.angle_range = [-90, 90]\n",
    "        self.angle_modifiers = [-15, 15]\n",
    "        \n",
    "        self._valid_actions = torch.full([self.grid_size[0] * self.grid_size[1] * len(self.angle_modifiers)], \n",
    "                                         True, dtype=torch.bool)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        def angle_matrix_action(i, j, mod):\n",
    "            def action(state):\n",
    "                state['angle_matrix'][i, j] = clip(state['angle_matrix'][i, j] + self.angle_modifiers[mod], \n",
    "                                                   self.angle_range[0], self.angle_range[1])\n",
    "                # If reach the boundry, update availbility status\n",
    "                if state['angle_matrix'][i, j] in self.angle_range:\n",
    "                    self._valid_actions[i * j + mod] = False\n",
    "                else:\n",
    "                    self._valid_actions[i * j + mod] = True\n",
    "                return state\n",
    "            return Action(f'({i}, {j})->{self.angle_modifiers[mod]}', action)\n",
    "        \n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                for mod in range(len(self.angle_modifiers)):\n",
    "                    self._action_space.append(angle_matrix_action(i, j, mod))\n",
    "\n",
    "                    \n",
    "    def reset(self) -> None: \n",
    "        self._state = TurnableGridState()\n",
    "        \n",
    "    def step(self, action_index: int) -> None: \n",
    "        action = self._action_space[action_index]\n",
    "        self._state = self._state.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEM-based Reward & Terminal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEMReward():\n",
    "    def __init__(self,\n",
    "                 target: Tensor,\n",
    "                 hyperparameters: Dict[str, Any]) -> None:\n",
    "\n",
    "        self.target = (target - 293.15) / 60\n",
    "        # exp['target_value'] = target\n",
    "\n",
    "        self.loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        self.goal_reward = hyperparameters['goal_reward']\n",
    "        self.terminal_error_threshold = hyperparameters['terminal_error_threshold']\n",
    "        self.loop_penalty = hyperparameters['loop_penalty']\n",
    "        self.invalid_state_penalty = hyperparameters['invalid_state_penalty']\n",
    "\n",
    "    def __call__(self, transition: SimulationTransition) -> SimulationTransition:\n",
    "        \"\"\"\n",
    "        Calculate reward value for a transition, and determine if a terminal state is reached\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition : SimulationTransition\n",
    "            A transition with completed simulation data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float | None\n",
    "            Reward value, None if the next_state is terminal\n",
    "        bool\n",
    "            The next_state is terminal\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        \n",
    "        if transition.state_sim and transition.state_sim['status'] == 'done':\n",
    "            state_result = torch.tensor(transition.state_sim['output']['temperature_distribution'][2])\n",
    "            transition.state_error = float(self.loss((state_result - 293.15) / 60, self.target))\n",
    "            transition.is_state_terminal = transition.state_error <= self.terminal_error_threshold\n",
    "        \n",
    "        if transition.next_state_sim and transition.next_state_sim['status'] == 'done':\n",
    "            next_state_result = torch.tensor(transition.next_state_sim['output']['temperature_distribution'][2])\n",
    "            transition.next_state_error = float(self.loss((next_state_result - 293.15) / 60, self.target))\n",
    "            transition.is_next_state_terminal = transition.next_state_error <= self.terminal_error_threshold\n",
    "        \n",
    "        transition.reward = 0.\n",
    "        \n",
    "        # Reward decreasing error from state to next state\n",
    "        transition.reward += transition.state_error - transition.next_state_error\n",
    "        \n",
    "        # # Negative reward based on next_state_error\n",
    "        # if transition.next_state_error:\n",
    "        #     transition.reward = -transition.next_state_error\n",
    "        # else:\n",
    "        #     transition.reward = self.invalid_state_penalty\n",
    "        \n",
    "        # If the state not changed after apply the action(s_n -> s_n),\n",
    "        # the loop penalty is applied\n",
    "        if transition.state_id == transition.next_state_id:\n",
    "            transition.reward = self.loop_penalty\n",
    "        \n",
    "\n",
    "        \n",
    "        # Reward extra if next state is final\n",
    "        if transition.is_next_state_terminal:\n",
    "            transition.reward = self.goal_reward\n",
    "        \n",
    "        return transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Network Container\n",
    "class Model():\n",
    "    def __init__(self, network: nn.Module, loss_func: _Loss, optimizer: Optimizer):\n",
    "        self.network = network\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __call__(self, network_input: Tensor) -> Tensor:\n",
    "        return self.network(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNet(state_size: int = 16, action_number: int = 32, target_network: bool = False):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(state_size, 100, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 200, device=cuda),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, action_number, device=cuda),\n",
    "    )\n",
    "    if target_network:\n",
    "        return Model(network=net, loss_func=None, optimizer=None)\n",
    "    else:\n",
    "        # exp['Network'] = str(torchinfo.summary(net, input_size=(32, state_size), \n",
    "        #                                        device=cuda, verbose=0))\n",
    "        return Model(network=net, loss_func=nn.SmoothL1Loss(), optimizer=torch.optim.Adam(net.parameters(), 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory: deque = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(ReplayTransition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, environment: Environment, simulator: SimHubClient, reward_func: RewardFunc, \n",
    "                 q_network: nn.Module, target_network: nn.Module, hyperparameters: Dict[str, Any]) -> None:\n",
    "        self.environment: Environment = environment\n",
    "        self.fem_simulator: SimHubClient = simulator\n",
    "        self.fem_reward_func: RewardFunc = reward_func\n",
    "        \n",
    "        self.q_network: Model = q_network\n",
    "        self.target_network: Model = target_network  \n",
    "        \n",
    "        self.target_update_interval: int = hyperparameters['target_update_interval']\n",
    "\n",
    "        self.optimization_iterations: int = hyperparameters['optimization_iterations']\n",
    "        self.max_step_per_episode: int = hyperparameters['max_step_per_episode']\n",
    "        self.experience_replay: ReplayMemory = ReplayMemory(hyperparameters['experience_replay_capacity'])\n",
    "        self.replay_batch_size: int = hyperparameters['replay_batch_size']\n",
    "\n",
    "        self.discount_factor: float = hyperparameters['discount_factor']\n",
    "        self.explore_factor_initial: float = hyperparameters['explore_factor_initial']\n",
    "        self.explore_factor_minimal: float = hyperparameters['explore_factor_minimal']\n",
    "        self.explore_factor_halflife: float = hyperparameters['explore_factor_halflife']\n",
    "        \n",
    "        self.pending_transitions: List[SimulationTransition] = list()\n",
    "        \n",
    "        # Set to true when generating result\n",
    "        self.generation_mode: bool = False\n",
    "        self.explored_step: int = 0\n",
    "        \n",
    "        self.episode: int = 0\n",
    "        self.step_num: int = 0\n",
    "        \n",
    "\n",
    "    \n",
    "    def select_action(self) -> Tuple[State, int, str]:\n",
    "        \"\"\"\n",
    "        Decide an action based on epsilon greedy algorithm\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        State\n",
    "            Current state instance\n",
    "        int\n",
    "            Index number of an action in the action space\n",
    "        \n",
    "        str\n",
    "            Action type, literal string of \"Prediction\" or \"Random\"\n",
    "        \"\"\"\n",
    "        state = self.environment.state\n",
    "        explore_factor = max(self.explore_factor_initial \n",
    "                             * (0.5 ** (self.explored_step / self.explore_factor_halflife)), \n",
    "                             self.explore_factor_minimal)\n",
    "        \n",
    "        if random.random() > explore_factor or self.generation_mode:\n",
    "            prediction = self.q_network(state.to_tensor().flatten())\n",
    "            # Mask invalid actions with with negative number\n",
    "            action_scores = prediction.flatten().masked_fill(self.environment.valid_actions, sys.float_info.min)\n",
    "            action_index = action_scores.argmax().item()\n",
    "            action_type = 'Prediction'\n",
    "            \n",
    "        else:\n",
    "            action_index = random.randrange(len(self.environment.action_space))\n",
    "            action_type = 'Random'\n",
    "            self.explored_step += 1\n",
    "        return state, action_index, action_type\n",
    "    \n",
    "    def step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Perform an action in the in the environment and submit the transition as FEM task to simulator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Return False if the episode ends earlier (a terminal state encountered), \n",
    "            Otherwise, return True to continue the current episode\n",
    "        \"\"\"\n",
    "        state, action_index, action_type = self.select_action()\n",
    "        self.environment.step(action_index)\n",
    "        next_state = self.environment.state\n",
    "        \n",
    "        \n",
    "        transition = SimulationTransition(self.episode, self.step_num, state, action_index, next_state)\n",
    "        transition.state_id, state_result = self.fem_simulator.submit_task(state)\n",
    "        transition.next_state_id, next_state_result = self.fem_simulator.submit_task(next_state)\n",
    "        \n",
    "        if DEBUG:\n",
    "            transition.action_name = self.environment.action_space[action_index].name\n",
    "        \n",
    "        # If both result already exsit, proceed to reward and memory instead of waiting\n",
    "        if state_result and next_state_result:\n",
    "            transition.state_sim = state_result\n",
    "            transition.next_state_sim = next_state_result\n",
    "            \n",
    "            self.compute_reward(transition)\n",
    "            return not transition.is_state_terminal\n",
    "        \n",
    "        self.pending_transitions.append(transition)\n",
    "        return True\n",
    "    \n",
    "    def compute_reward(self, transition: SimulationTransition) -> None:\n",
    "        \"\"\"\n",
    "        Compute reward value and terminal status for a COMPLETED transition. \n",
    "        The states, action and reward will be pushed into experience replay\n",
    "        \n",
    "        If the current state is terminal, transition.next_state will be set to None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        SimulationTransition\n",
    "            Return transition\n",
    "        \"\"\"\n",
    "        self.fem_reward_func(transition)\n",
    "        \n",
    "        if DEBUG and not self.generation_mode:\n",
    "            exp[f'transitions/{transition.episode}'].append(str(transition))\n",
    "\n",
    "        if not self.generation_mode:\n",
    "            self.experience_replay.push(transition.state.to_tensor(), \n",
    "                                        transition.action_index, \n",
    "                                        transition.reward, \n",
    "                                        None if transition.is_state_terminal else transition.next_state.to_tensor())\n",
    "\n",
    "        \n",
    "    def compute_pending_rewards(self) -> None:\n",
    "        self.fem_simulator.wait()\n",
    "\n",
    "        while len(self.pending_transitions) > 0:\n",
    "            transition: SimulationTransition = self.pending_transitions.pop(0)\n",
    "\n",
    "            transition.state_sim = self.fem_simulator.get_result(transition.state_id)\n",
    "            transition.next_state_sim = self.fem_simulator.get_result(transition.next_state_id)\n",
    "            \n",
    "            self.compute_reward(transition)\n",
    "            \n",
    "            # Skip all remaining transition beyond terminal state\n",
    "            if transition.is_state_terminal:\n",
    "                break\n",
    "        \n",
    "    def optimize(self) -> None:\n",
    "        if len(self.experience_replay) < self.replay_batch_size: return\n",
    "\n",
    "        samples = self.experience_replay.sample(self.replay_batch_size)\n",
    "        batch = ReplayTransition(*zip(*samples))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                  batch.next_state)), device=cuda, dtype=torch.bool)\n",
    "        # If none of the transition has a valid next_step, skip the round\n",
    "        if not non_final_mask.any():\n",
    "            return\n",
    "        non_final_next_states = torch.stack([s.flatten() for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "\n",
    "        state_batch = torch.stack([s.flatten() for s in batch.state])\n",
    "        action_batch = torch.tensor(batch.action_index, device=cuda).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(batch.reward, device=cuda)\n",
    "\n",
    "        state_action_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(self.replay_batch_size, device=cuda)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        loss = self.q_network.loss_func(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        optimization_loss = float(loss)\n",
    "        self.q_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.q_network.network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.q_network.optimizer.step()\n",
    "        return optimization_loss\n",
    "        \n",
    "    def update_target_network(self) -> None:\n",
    "        self.target_network.network.load_state_dict(self.q_network.network.state_dict())\n",
    "        \n",
    "    def train(self, episodes: int) -> None:\n",
    "        for episode in range(episodes):\n",
    "            print('')\n",
    "            print(f'Episode: {episode}')\n",
    "            self.episode = episode\n",
    "            \n",
    "            for self.step_num in range(self.max_step_per_episode):\n",
    "                if not self.step():\n",
    "                    break\n",
    "            self.compute_pending_rewards()\n",
    "            \n",
    "            for i in range(self.optimization_iterations):\n",
    "                loss = self.optimize()\n",
    "                exp['optimization_loss'].append(loss, step=self.episode + i / self.optimization_iterations)\n",
    "                \n",
    "            self.fem_simulator.clear_tasks()\n",
    "            if episode % self.target_update_interval == 0:\n",
    "                self.update_target_network()\n",
    "                \n",
    "            if DEBUG:\n",
    "                print('Generating...')\n",
    "                generated_state, generated_result = self.generate()\n",
    "                \n",
    "                exp['generated_state'].append(str(generated_state), step=self.episode)\n",
    "                \n",
    "                result_size = (len(np.unique(generated_result[0])), len(np.unique(generated_result[1])))\n",
    "                plt.imshow(generated_result[2].reshape(result_size))\n",
    "                exp['generated_result'].append(plt.gcf(), step=self.episode)\n",
    "                plt.close()\n",
    "                \n",
    "                print('Done')\n",
    "            \n",
    "        \n",
    "    def generate(self) -> State:\n",
    "        self.generation_mode = True\n",
    "        for self.step_num in range(self.max_step_per_episode):\n",
    "            if not self.step():\n",
    "                break\n",
    "        \n",
    "        # It is possible that the result is an unknown state\n",
    "        state_id, state_result = self.fem_simulator.submit_task(self.environment.state)\n",
    "        self.fem_simulator.wait(print_stats=False, progress_bar=False)\n",
    "        self.fem_simulator.clear_tasks()\n",
    "        self.generation_mode = False\n",
    "        return self.environment.state, self.fem_simulator.get_result(state_id)['output']['temperature_distribution']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nwen/metamaterial-rl/elmer_task/elmer_script.py\n",
      "/home/nwen/metamaterial-rl/elmer_task/data\n",
      "Establishing working directory structure...\n",
      "Working directory structure established\n",
      "Copying script files...\n",
      "Copying /home/nwen/metamaterial-rl/elmer_task/elmer_script.py\n",
      "/home/nwen/metamaterial-rl/elmer_task/elmer_script.py copied\n",
      "Copying data files...\n",
      "Copying /home/nwen/metamaterial-rl/elmer_task/data\n",
      "/home/nwen/metamaterial-rl/elmer_task/data copied\n",
      "Entry script set to /scratch1/nwen/simhub/workspaces/scripts/elmer_script.py\n"
     ]
    }
   ],
   "source": [
    "env = TurnableGridEnvironment()\n",
    "\n",
    "fem = SimHubClient('10.128.97.115', 44444, database_ip='10.125.9.35')\n",
    "fem.set_experiment('./elmer_task/elmer_task.yml')\n",
    "\n",
    "target_arr = np.load('target.npy')\n",
    "target_size = (len(np.unique(target_arr[0])), len(np.unique(target_arr[1])))\n",
    "plt.imshow(target_arr[2].reshape(target_size))\n",
    "exp['target'] = plt.gcf()\n",
    "plt.close()\n",
    "\n",
    "reward_func = FEMReward(torch.tensor(target_arr[2]), hyperparameters)\n",
    "\n",
    "agent = Agent(env, fem, reward_func, QNet(), QNet(target_network=True), hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 0\n",
      "Existed: 2(100.00%)\n",
      "Duplicated: 0(0.00%)\n",
      "New: 0(0.00%)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012893438339233398,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9158ec62f6334322a9d462a60ff12d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful: 2(100.00%)\n",
      "Failed: 0(0.00%)\n",
      "Generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.105119.pbs02/ipykernel_3181230/3694325163.py:188: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `stringify_unsupported(collection)` for collections and dictionaries. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  exp['optimization_loss'].append(loss, step=self.episode + i / self.optimization_iterations)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#print(agent.generate())\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m     generated_state, generated_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_state\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(generated_state), step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode)\n\u001b[1;32m    200\u001b[0m     result_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(generated_result[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(generated_result[\u001b[38;5;241m1\u001b[39m])))\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mAgent.generate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# It is possible that the result is an unknown state\u001b[39;00m\n\u001b[1;32m    215\u001b[0m state_id, state_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfem_simulator\u001b[38;5;241m.\u001b[39msubmit_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfem_simulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfem_simulator\u001b[38;5;241m.\u001b[39mclear_tasks()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/metamaterial-rl/SimHubClient.py:321\u001b[0m, in \u001b[0;36mSimHubClient.wait\u001b[0;34m(self, interval, print_stats, progress_bar)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results[i]:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmitted_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m input_hash, status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmitted_tasks[i]\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_hash \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_cache:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(agent.generate())\n",
    "\n",
    "agent.train(1000)\n",
    "\n",
    "#print(agent.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/pil-clemson/metamtl-rl/e/METAMTLRL-168\n"
     ]
    }
   ],
   "source": [
    "exp.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem = SimHubClient('10.128.97.115', 44444, database_ip='10.125.9.35')\n",
    "fem.set_experiment('./elmer_task/elmer_task.yml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TurnableGridState()\n",
    "state['angle_matrix'] = np.array(\n",
    "    [[45, 80, -80, -45], \n",
    "     [10, 45, -45, -10], \n",
    "     [-10, -45, 45, 10], \n",
    "     [-45, -80, 80, 45]]\n",
    ")\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = fem.submit_task(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('target.npy', fem.get_result(task[0])['output']['temperature_distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.load('target.npy')[2].reshape(40, 40))\n",
    "plt.gcf().savefig('target.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np.load('target.npy')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyRL3.10",
   "language": "python",
   "name": "pyrl3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "neptune": {
   "notebookId": "45d03d69-6ac7-41ca-8af8-80caaa73aad5",
   "projectVersion": 2
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
